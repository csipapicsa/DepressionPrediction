{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader, Dataset\n",
    "import torch.nn as nn\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "\n",
    "### load data and labels\n",
    "### mfcc has various lengths \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "\n",
    "from imblearn.under_sampling import RandomUnderSampler\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before balancing: [4378 1781]\n",
      "After balancing: [1781 1781]\n",
      "Before balancing: [967 712]\n",
      "After balancing: [712 712]\n"
     ]
    }
   ],
   "source": [
    "def load_mfcc(path):\n",
    "    \n",
    "    # Load the concatenated MFCC data\n",
    "    data = np.load(path)\n",
    "    return data['X'], data['y']\n",
    "\n",
    "X_train, y_train = load_mfcc('DAIC_train.npz')\n",
    "X_test, y_test = load_mfcc('DAIC_test.npz')\n",
    "\n",
    "# Flatten the MFCC data from 3D to 2D for resampling\n",
    "X_train_flattened = X_train.reshape(X_train.shape[0], -1)\n",
    "X_test_flattened = X_test.reshape(X_test.shape[0], -1)\n",
    "\n",
    "# Initialize the RandomUnderSampler\n",
    "rus = RandomUnderSampler(random_state=42)\n",
    "\n",
    "# Resample the dataset\n",
    "X_train_resampled, y_train_resampled = rus.fit_resample(X_train_flattened, y_train)\n",
    "X_test_resampled, y_test_resampled = rus.fit_resample(X_test_flattened, y_test)\n",
    "\n",
    "# Check the balance\n",
    "print(\"Before balancing:\", np.bincount(y_train))\n",
    "print(\"After balancing:\", np.bincount(y_train_resampled))\n",
    "print(\"Before balancing:\", np.bincount(y_test))\n",
    "print(\"After balancing:\", np.bincount(y_test_resampled))\n",
    "\n",
    "# Optional: Reshape back to 3D if your model requires\n",
    "X_train_resampled_3d = X_train_resampled.reshape(-1, 40, 200)  # Modify dimensions as necessary\n",
    "X_test_resampled_3d = X_test_resampled.reshape(-1, 40, 200)  # Modify dimensions as necessary\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## check zeros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Indices of matrices that are entirely zero: []\n",
      "Indices and rows of matrices that have zero rows: []\n",
      "Number of zero elements in each matrix: [3360, 2360]\n"
     ]
    }
   ],
   "source": [
    "def check_zeros(X):\n",
    "    all_zero_matrices = []\n",
    "    zero_rows_matrices = []\n",
    "    zero_elements_count = []\n",
    "\n",
    "    for i, matrix in enumerate(X):\n",
    "        # Check if the entire matrix is zero\n",
    "        if np.all(matrix == 0):\n",
    "            all_zero_matrices.append(i)\n",
    "        \n",
    "        # Check for zero rows in the matrix\n",
    "        zero_rows = [idx for idx, row in enumerate(matrix) if np.all(row == 0)]\n",
    "        if zero_rows:\n",
    "            zero_rows_matrices.append((i, zero_rows))\n",
    "        \n",
    "        # Count zero elements\n",
    "        zero_elements_count.append(np.sum(matrix == 0))\n",
    "    \n",
    "    return all_zero_matrices, zero_rows_matrices, zero_elements_count\n",
    "\n",
    "# Load your data\n",
    "X_train, y_train = load_mfcc('DAIC_train.npz')\n",
    "\n",
    "# Check for zeros\n",
    "all_zeros, zero_rows, zero_elements = check_zeros(X_train[0:2])\n",
    "\n",
    "# Print results\n",
    "print(f\"Indices of matrices that are entirely zero: {all_zeros}\")\n",
    "print(f\"Indices and rows of matrices that have zero rows: {zero_rows}\")\n",
    "print(f\"Number of zero elements in each matrix: {zero_elements}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AudioDataset(Dataset):\n",
    "    def __init__(self, mfcc_features, labels):\n",
    "        self.features = torch.FloatTensor(mfcc_features)\n",
    "        self.labels = torch.LongTensor(labels)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        return self.features[idx], self.labels[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# shuffle X_train_resampled_3d and y_train_resampled, and split into train and validation sets\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train_resampled_3d, X_val_resampled_3d, y_train_resampled, y_val_resampled = train_test_split(X_train_resampled_3d, y_train_resampled, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After balancing: [1781 1781]\n",
      "After balancing: [712 712]\n"
     ]
    }
   ],
   "source": [
    "print(\"After balancing:\", np.bincount(y_train_resampled))\n",
    "print(\"After balancing:\", np.bincount(y_test_resampled))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_train = AudioDataset(X_train_resampled_3d, y_train_resampled)\n",
    "dataset_val = AudioDataset(X_test_resampled_3d, y_test_resampled)\n",
    "\n",
    "# use just the train\n",
    "# dataset_train = AudioDataset(X_train_resampled_3d, y_train_resampled)\n",
    "# dataset_val = AudioDataset(X_val_resampled_3d, y_val_resampled)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The model and training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TCC(nn.Module):\n",
    "    def __init__(self, input_features=40, sequence_length=200):\n",
    "        super(TCC, self).__init__()\n",
    "        \n",
    "        # Parallel CNN streams (as per paper Section 3.1)\n",
    "        self.cnn_stream1 = CNNStream()\n",
    "        self.cnn_stream2 = CNNStream()\n",
    "        \n",
    "        # Transformer stream with linear attention\n",
    "        self.transformer_stream = TransformerStream(input_features, sequence_length)\n",
    "        \n",
    "        # Fusion layer\n",
    "        cnn_out_size = 64 * (sequence_length // 8)  # After 3 max pooling layers\n",
    "        transformer_out_size = 512 # not used anymore\n",
    "\n",
    "\n",
    "        # total_features = (cnn_out_size * 2) + transformer_out_size\n",
    "        total_features = (cnn_out_size * 2) + 128  # reduced from 512\n",
    "\n",
    "        \n",
    "        self.fusion = nn.Sequential(\n",
    "            nn.Linear(total_features, 128), #reduce the number of features 512 was\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.4),\n",
    "            nn.Linear(128, 2)  # Binary classification # reduced from 512, 256, 128\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Parallel processing through streams\n",
    "        cnn1_out = self.cnn_stream1(x)\n",
    "        cnn2_out = self.cnn_stream2(x)\n",
    "        transformer_out = self.transformer_stream(x)\n",
    "        \n",
    "        # Flatten and concatenate\n",
    "        cnn1_out = cnn1_out.flatten(1)\n",
    "        cnn2_out = cnn2_out.flatten(1)\n",
    "        \n",
    "        # Fusion\n",
    "        combined = torch.cat([cnn1_out, cnn2_out, transformer_out], dim=1)\n",
    "        output = self.fusion(combined)\n",
    "        \n",
    "        return output\n",
    "\n",
    "class CNNStream(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNNStream, self).__init__()\n",
    "        \n",
    "        # Three conv layers as specified in paper\n",
    "        self.layers = nn.Sequential(\n",
    "            # First conv block\n",
    "            nn.Conv1d(40, 16, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool1d(2),\n",
    "            nn.BatchNorm1d(16),\n",
    "            nn.Dropout(0.5),\n",
    "            \n",
    "            # Second conv block\n",
    "            nn.Conv1d(16, 32, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool1d(2),\n",
    "            nn.BatchNorm1d(32),\n",
    "            nn.Dropout(0.5),\n",
    "            \n",
    "            # Third conv block\n",
    "            nn.Conv1d(32, 64, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool1d(2),\n",
    "            nn.BatchNorm1d(64),\n",
    "            nn.Dropout(0.5)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.layers(x)\n",
    "\n",
    "class TransformerStream(nn.Module):\n",
    "    def __init__(self, input_features, sequence_length):\n",
    "        super(TransformerStream, self).__init__()\n",
    "        \n",
    "        self.input_proj = nn.Linear(input_features, 128) # 265 instead of 512\n",
    "        \n",
    "        # Four transformer layers with linear attention\n",
    "        self.transformer_layers = nn.ModuleList([\n",
    "            TransformerLayer(d_model=128) for _ in range(4) # reduced to 256 from 512 range 4-> 2\n",
    "        ])\n",
    "        \n",
    "        # Output pooling\n",
    "        self.pool = nn.AdaptiveAvgPool1d(1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Transform input: [batch, features, seq] -> [batch, seq, features]\n",
    "        x = x.transpose(1, 2)\n",
    "        \n",
    "        # Project to transformer dimension\n",
    "        x = self.input_proj(x)\n",
    "        \n",
    "        # Apply transformer layers\n",
    "        for layer in self.transformer_layers:\n",
    "            x = layer(x)\n",
    "        \n",
    "        # Global pooling over sequence dimension\n",
    "        x = x.transpose(1, 2)  # [batch, features, seq]\n",
    "        x = self.pool(x).squeeze(-1)  # [batch, features]\n",
    "        \n",
    "        return x\n",
    "\n",
    "class TransformerLayer(nn.Module):\n",
    "    def __init__(self, d_model=512):\n",
    "        super(TransformerLayer, self).__init__()\n",
    "        \n",
    "        # Linear attention with 4 heads\n",
    "        self.attention = LinearAttention(d_model, n_heads=4)\n",
    "        \n",
    "        # Feed-forward network\n",
    "        self.ffn = nn.Sequential(\n",
    "            nn.Linear(d_model, 1024), # 2048\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.4),\n",
    "            nn.Linear(1024, d_model) # 2048? \n",
    "        )\n",
    "        \n",
    "        # Layer norms\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Multi-head attention with residual\n",
    "        attended = self.attention(x)\n",
    "        x = self.norm1(x + attended)\n",
    "        \n",
    "        # Feed-forward with residual\n",
    "        ffn_out = self.ffn(x)\n",
    "        x = self.norm2(x + ffn_out)\n",
    "        \n",
    "        return x\n",
    "\n",
    "class LinearAttention(nn.Module):\n",
    "    def __init__(self, d_model, n_heads=4):\n",
    "        super(LinearAttention, self).__init__()\n",
    "        assert d_model % n_heads == 0\n",
    "        \n",
    "        self.d_model = d_model\n",
    "        self.n_heads = n_heads\n",
    "        self.d_head = d_model // n_heads\n",
    "        \n",
    "        # Linear projections\n",
    "        self.q_proj = nn.Linear(d_model, d_model)\n",
    "        self.k_proj = nn.Linear(d_model, d_model)\n",
    "        self.v_proj = nn.Linear(d_model, d_model)\n",
    "        self.o_proj = nn.Linear(d_model, d_model)\n",
    "        \n",
    "        self.elu = nn.ELU()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        batch_size, seq_len, _ = x.size()\n",
    "        \n",
    "        # Linear projections and reshape to heads\n",
    "        q = self.q_proj(x).view(batch_size, seq_len, self.n_heads, self.d_head)\n",
    "        k = self.k_proj(x).view(batch_size, seq_len, self.n_heads, self.d_head)\n",
    "        v = self.v_proj(x).view(batch_size, seq_len, self.n_heads, self.d_head)\n",
    "        \n",
    "        # Apply ELU + 1 for positive values (as per paper)\n",
    "        q = self.elu(q) + 1\n",
    "        k = self.elu(k) + 1\n",
    "        \n",
    "        # Linear attention computation\n",
    "        k_cumsum = k.sum(dim=1, keepdim=True)\n",
    "        D_inv = 1.0 / torch.einsum('bhnd,bhnd->bhn', q, k_cumsum)\n",
    "        context = torch.einsum('bhnd,bhne->bhde', k, v)\n",
    "        out = torch.einsum('bhnd,bhde,bhn->bhne', q, context, D_inv)\n",
    "        \n",
    "        # Reshape and project output\n",
    "        out = out.contiguous().view(batch_size, seq_len, self.d_model)\n",
    "        out = self.o_proj(out)\n",
    "        \n",
    "        return out\n",
    "\n",
    "def train_model(model, train_loader, val_loader, num_epochs=200, learning_rate=0.001, global_history=None, patience=21):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"Using device: {device}\")\n",
    "    \n",
    "    model = model.to(device)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay=0.01)\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=10, verbose=True)\n",
    "    \n",
    "    best_val_acc = 0\n",
    "    patience_counter = 0\n",
    "\n",
    "        # Initialize metric collectors\n",
    "    global_history = {\n",
    "        'train_loss': [],\n",
    "        'val_loss': [],\n",
    "        'train_acc': [],\n",
    "        'val_acc': []\n",
    "    }\n",
    "\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}\")\n",
    "        # Training\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        train_correct = 0\n",
    "        train_total = 0\n",
    "        \n",
    "        for features, labels in train_loader:\n",
    "            features, labels = features.to(device), labels.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(features)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "            optimizer.step()\n",
    "            \n",
    "            train_loss += loss.item()\n",
    "            _, predicted = outputs.max(1)\n",
    "            train_total += labels.size(0)\n",
    "            train_correct += predicted.eq(labels).sum().item()\n",
    "        \n",
    "        # Validation\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        val_correct = 0\n",
    "        val_total = 0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for features, labels in val_loader:\n",
    "                features, labels = features.to(device), labels.to(device)\n",
    "                outputs = model(features)\n",
    "                loss = criterion(outputs, labels)\n",
    "                \n",
    "                val_loss += loss.item()\n",
    "                _, predicted = outputs.max(1)\n",
    "                val_total += labels.size(0)\n",
    "                val_correct += predicted.eq(labels).sum().item()\n",
    "        \n",
    "        # Calculate metrics\n",
    "        train_acc = 100. * train_correct / train_total\n",
    "        val_acc = 100. * val_correct / val_total\n",
    "        avg_train_loss = train_loss / len(train_loader)\n",
    "        avg_val_loss = val_loss / len(val_loader)\n",
    "\n",
    "        global_history['train_loss'].append(avg_train_loss*100)\n",
    "        global_history['val_loss'].append(avg_val_loss*100)\n",
    "        global_history['train_acc'].append(train_acc)\n",
    "        global_history['val_acc'].append(val_acc)\n",
    "\n",
    "        with open('history_TCC_inside_balanced_FINAL.pkl', 'wb') as f:\n",
    "            pickle.dump(global_history, f)\n",
    "\n",
    "        print(f'Train Loss: {avg_train_loss:.4f}, Train Acc: {train_acc:.2f}%')\n",
    "        # validation loss aand accuracy print\n",
    "        print(f'Val Loss: {avg_val_loss:.4f}, Val Acc: {val_acc:.2f}%\\n')\n",
    "        print(40 * '-')\n",
    "        \n",
    "        # Update learning rate\n",
    "        scheduler.step(avg_val_loss)\n",
    "        \n",
    "        # Early stopping\n",
    "        if val_acc > best_val_acc:\n",
    "            print(40 * '-')\n",
    "            print('Model is saved')\n",
    "            print(40 * '-')\n",
    "            best_val_acc = val_acc\n",
    "            patience_counter = 0\n",
    "            torch.save(model.state_dict(), 'best_model_TCC_inside_balanced_FINAL.pth')\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            if patience_counter >= patience:\n",
    "                print(f\"Early stopping triggered at epoch {epoch+1}\")\n",
    "                # break\n",
    "        \n",
    "        if (epoch + 1) % 10 == 0:\n",
    "            print(f'Epoch [{epoch+1}/{num_epochs}]')\n",
    "            print(f'Train Loss: {avg_train_loss:.4f}, Train Acc: {train_acc:.2f}%')\n",
    "            print(f'Val Loss: {avg_val_loss:.4f}, Val Acc: {val_acc:.2f}%\\n')\n",
    "    \n",
    "    # Load best model\n",
    "    # model.load_state_dict(torch.load('best_model.pth'))\n",
    "    return model, global_history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Epoch 1/500\n",
      "Train Loss: 0.7334, Train Acc: 48.88%\n",
      "Val Loss: 0.6957, Val Acc: 49.79%\n",
      "\n",
      "----------------------------------------\n",
      "Epoch 2/500\n",
      "Train Loss: 0.7168, Train Acc: 51.15%\n",
      "Val Loss: 0.6976, Val Acc: 49.58%\n",
      "\n",
      "----------------------------------------\n",
      "Epoch 3/500\n",
      "Train Loss: 0.7027, Train Acc: 53.43%\n",
      "Val Loss: 0.7022, Val Acc: 50.21%\n",
      "\n",
      "----------------------------------------\n",
      "Epoch 4/500\n",
      "Train Loss: 0.6897, Train Acc: 54.35%\n",
      "Val Loss: 0.7005, Val Acc: 50.28%\n",
      "\n",
      "----------------------------------------\n",
      "Epoch 5/500\n",
      "Train Loss: 0.6899, Train Acc: 54.41%\n",
      "Val Loss: 0.7066, Val Acc: 50.21%\n",
      "\n",
      "----------------------------------------\n",
      "Epoch 6/500\n",
      "Train Loss: 0.6797, Train Acc: 56.82%\n",
      "Val Loss: 0.7173, Val Acc: 49.93%\n",
      "\n",
      "----------------------------------------\n",
      "Epoch 7/500\n",
      "Train Loss: 0.6765, Train Acc: 57.94%\n",
      "Val Loss: 0.7230, Val Acc: 50.00%\n",
      "\n",
      "----------------------------------------\n",
      "Epoch 8/500\n",
      "Train Loss: 0.6711, Train Acc: 57.58%\n",
      "Val Loss: 0.7214, Val Acc: 49.93%\n",
      "\n",
      "----------------------------------------\n",
      "Epoch 9/500\n",
      "Train Loss: 0.6599, Train Acc: 59.94%\n",
      "Val Loss: 0.7224, Val Acc: 50.07%\n",
      "\n",
      "----------------------------------------\n",
      "Epoch 10/500\n",
      "Train Loss: 0.6560, Train Acc: 61.37%\n",
      "Val Loss: 0.7210, Val Acc: 50.28%\n",
      "\n",
      "----------------------------------------\n",
      "Epoch [10/500]\n",
      "Train Loss: 0.6560, Train Acc: 61.37%\n",
      "Val Loss: 0.7210, Val Acc: 50.28%\n",
      "\n",
      "Epoch 11/500\n",
      "Train Loss: 0.6485, Train Acc: 62.35%\n",
      "Val Loss: 0.7413, Val Acc: 49.93%\n",
      "\n",
      "----------------------------------------\n",
      "Epoch 12/500\n",
      "Train Loss: 0.6394, Train Acc: 63.25%\n",
      "Val Loss: 0.7389, Val Acc: 50.35%\n",
      "\n",
      "----------------------------------------\n",
      "Epoch 13/500\n",
      "Train Loss: 0.6325, Train Acc: 64.09%\n",
      "Val Loss: 0.7388, Val Acc: 50.21%\n",
      "\n",
      "----------------------------------------\n",
      "Epoch 14/500\n",
      "Train Loss: 0.6319, Train Acc: 64.07%\n",
      "Val Loss: 0.7528, Val Acc: 50.14%\n",
      "\n",
      "----------------------------------------\n",
      "Epoch 15/500\n",
      "Train Loss: 0.6305, Train Acc: 63.90%\n",
      "Val Loss: 0.7463, Val Acc: 50.28%\n",
      "\n",
      "----------------------------------------\n",
      "Epoch 16/500\n",
      "Train Loss: 0.6188, Train Acc: 63.98%\n",
      "Val Loss: 0.7407, Val Acc: 50.07%\n",
      "\n",
      "----------------------------------------\n",
      "Epoch 17/500\n",
      "Train Loss: 0.6198, Train Acc: 64.37%\n",
      "Val Loss: 0.7466, Val Acc: 50.21%\n",
      "\n",
      "----------------------------------------\n",
      "Epoch 18/500\n",
      "Train Loss: 0.6196, Train Acc: 64.35%\n",
      "Val Loss: 0.7438, Val Acc: 50.00%\n",
      "\n",
      "----------------------------------------\n",
      "Epoch 19/500\n",
      "Train Loss: 0.6129, Train Acc: 66.31%\n",
      "Val Loss: 0.7360, Val Acc: 50.56%\n",
      "\n",
      "----------------------------------------\n",
      "Epoch 20/500\n",
      "Train Loss: 0.6067, Train Acc: 66.34%\n",
      "Val Loss: 0.7349, Val Acc: 50.63%\n",
      "\n",
      "----------------------------------------\n",
      "Epoch [20/500]\n",
      "Train Loss: 0.6067, Train Acc: 66.34%\n",
      "Val Loss: 0.7349, Val Acc: 50.63%\n",
      "\n",
      "Epoch 21/500\n",
      "Train Loss: 0.6074, Train Acc: 66.31%\n",
      "Val Loss: 0.7469, Val Acc: 50.35%\n",
      "\n",
      "----------------------------------------\n",
      "Epoch 22/500\n",
      "Train Loss: 0.6045, Train Acc: 66.70%\n",
      "Val Loss: 0.7455, Val Acc: 50.42%\n",
      "\n",
      "----------------------------------------\n",
      "Epoch 23/500\n",
      "Train Loss: 0.5979, Train Acc: 66.84%\n",
      "Val Loss: 0.7374, Val Acc: 50.42%\n",
      "\n",
      "----------------------------------------\n",
      "Epoch 24/500\n",
      "Train Loss: 0.5960, Train Acc: 67.21%\n",
      "Val Loss: 0.7415, Val Acc: 50.49%\n",
      "\n",
      "----------------------------------------\n",
      "Epoch 25/500\n",
      "Train Loss: 0.5902, Train Acc: 67.04%\n",
      "Val Loss: 0.7362, Val Acc: 50.91%\n",
      "\n",
      "----------------------------------------\n",
      "Epoch 26/500\n",
      "Train Loss: 0.5881, Train Acc: 67.43%\n",
      "Val Loss: 0.7431, Val Acc: 50.56%\n",
      "\n",
      "----------------------------------------\n",
      "Epoch 27/500\n",
      "Train Loss: 0.5914, Train Acc: 68.25%\n",
      "Val Loss: 0.7571, Val Acc: 50.70%\n",
      "\n",
      "----------------------------------------\n",
      "Epoch 28/500\n",
      "Train Loss: 0.5912, Train Acc: 66.84%\n",
      "Val Loss: 0.7438, Val Acc: 50.49%\n",
      "\n",
      "----------------------------------------\n",
      "Epoch 29/500\n",
      "Train Loss: 0.5867, Train Acc: 68.61%\n",
      "Val Loss: 0.7372, Val Acc: 50.70%\n",
      "\n",
      "----------------------------------------\n",
      "Epoch 30/500\n",
      "Train Loss: 0.5872, Train Acc: 68.25%\n",
      "Val Loss: 0.7418, Val Acc: 50.49%\n",
      "\n",
      "----------------------------------------\n",
      "Epoch [30/500]\n",
      "Train Loss: 0.5872, Train Acc: 68.25%\n",
      "Val Loss: 0.7418, Val Acc: 50.49%\n",
      "\n",
      "Epoch 31/500\n",
      "Train Loss: 0.5753, Train Acc: 68.64%\n",
      "Val Loss: 0.7412, Val Acc: 50.91%\n",
      "\n",
      "----------------------------------------\n",
      "Epoch 32/500\n",
      "Train Loss: 0.5742, Train Acc: 69.96%\n",
      "Val Loss: 0.7556, Val Acc: 50.77%\n",
      "\n",
      "----------------------------------------\n",
      "Epoch 33/500\n",
      "Train Loss: 0.5757, Train Acc: 69.62%\n",
      "Val Loss: 0.7424, Val Acc: 50.98%\n",
      "\n",
      "----------------------------------------\n",
      "Epoch 34/500\n",
      "Train Loss: 0.5756, Train Acc: 68.78%\n",
      "Val Loss: 0.7449, Val Acc: 50.91%\n",
      "\n",
      "----------------------------------------\n",
      "Epoch 35/500\n",
      "Train Loss: 0.5750, Train Acc: 68.73%\n",
      "Val Loss: 0.7478, Val Acc: 50.91%\n",
      "\n",
      "----------------------------------------\n",
      "Epoch 36/500\n",
      "Train Loss: 0.5715, Train Acc: 68.30%\n",
      "Val Loss: 0.7515, Val Acc: 50.56%\n",
      "\n",
      "----------------------------------------\n",
      "Epoch 37/500\n",
      "Train Loss: 0.5744, Train Acc: 69.06%\n",
      "Val Loss: 0.7505, Val Acc: 50.84%\n",
      "\n",
      "----------------------------------------\n",
      "Epoch 38/500\n",
      "Train Loss: 0.5660, Train Acc: 69.85%\n",
      "Val Loss: 0.7558, Val Acc: 50.56%\n",
      "\n",
      "----------------------------------------\n",
      "Epoch 39/500\n",
      "Train Loss: 0.5722, Train Acc: 69.06%\n",
      "Val Loss: 0.7465, Val Acc: 51.05%\n",
      "\n",
      "----------------------------------------\n",
      "Epoch 40/500\n",
      "Train Loss: 0.5778, Train Acc: 68.67%\n",
      "Val Loss: 0.7485, Val Acc: 50.91%\n",
      "\n",
      "----------------------------------------\n",
      "Epoch [40/500]\n",
      "Train Loss: 0.5778, Train Acc: 68.67%\n",
      "Val Loss: 0.7485, Val Acc: 50.91%\n",
      "\n",
      "Epoch 41/500\n",
      "Train Loss: 0.5645, Train Acc: 70.72%\n",
      "Val Loss: 0.7421, Val Acc: 51.12%\n",
      "\n",
      "----------------------------------------\n",
      "Epoch 42/500\n",
      "Train Loss: 0.5664, Train Acc: 69.26%\n",
      "Val Loss: 0.7499, Val Acc: 50.84%\n",
      "\n",
      "----------------------------------------\n",
      "Epoch 43/500\n",
      "Train Loss: 0.5699, Train Acc: 70.07%\n",
      "Val Loss: 0.7469, Val Acc: 51.05%\n",
      "\n",
      "----------------------------------------\n",
      "Epoch 44/500\n",
      "Train Loss: 0.5619, Train Acc: 70.33%\n",
      "Val Loss: 0.7461, Val Acc: 51.05%\n",
      "\n",
      "----------------------------------------\n",
      "Epoch 45/500\n",
      "Train Loss: 0.5650, Train Acc: 69.60%\n",
      "Val Loss: 0.7576, Val Acc: 50.84%\n",
      "\n",
      "----------------------------------------\n",
      "Epoch 46/500\n",
      "Train Loss: 0.5692, Train Acc: 69.54%\n",
      "Val Loss: 0.7539, Val Acc: 50.70%\n",
      "\n",
      "----------------------------------------\n",
      "Epoch 47/500\n",
      "Train Loss: 0.5585, Train Acc: 70.33%\n",
      "Val Loss: 0.7522, Val Acc: 50.84%\n",
      "\n",
      "----------------------------------------\n",
      "Epoch 48/500\n",
      "Train Loss: 0.5629, Train Acc: 70.30%\n",
      "Val Loss: 0.7525, Val Acc: 50.63%\n",
      "\n",
      "----------------------------------------\n",
      "Epoch 49/500\n",
      "Train Loss: 0.5472, Train Acc: 71.87%\n",
      "Val Loss: 0.7497, Val Acc: 50.77%\n",
      "\n",
      "----------------------------------------\n",
      "Epoch 50/500\n",
      "Train Loss: 0.5553, Train Acc: 71.36%\n",
      "Val Loss: 0.7470, Val Acc: 50.84%\n",
      "\n",
      "----------------------------------------\n",
      "Epoch [50/500]\n",
      "Train Loss: 0.5553, Train Acc: 71.36%\n",
      "Val Loss: 0.7470, Val Acc: 50.84%\n",
      "\n",
      "Epoch 51/500\n",
      "Train Loss: 0.5560, Train Acc: 70.35%\n",
      "Val Loss: 0.7499, Val Acc: 51.19%\n",
      "\n",
      "----------------------------------------\n",
      "Epoch 52/500\n",
      "Train Loss: 0.5550, Train Acc: 70.38%\n",
      "Val Loss: 0.7541, Val Acc: 50.70%\n",
      "\n",
      "----------------------------------------\n",
      "Epoch 53/500\n",
      "Train Loss: 0.5570, Train Acc: 70.61%\n",
      "Val Loss: 0.7544, Val Acc: 50.84%\n",
      "\n",
      "----------------------------------------\n",
      "Epoch 54/500\n",
      "Train Loss: 0.5582, Train Acc: 70.97%\n",
      "Val Loss: 0.7547, Val Acc: 50.84%\n",
      "\n",
      "----------------------------------------\n",
      "Epoch 55/500\n",
      "Train Loss: 0.5559, Train Acc: 70.97%\n",
      "Val Loss: 0.7497, Val Acc: 51.12%\n",
      "\n",
      "----------------------------------------\n",
      "Epoch 56/500\n",
      "Train Loss: 0.5570, Train Acc: 70.97%\n",
      "Val Loss: 0.7558, Val Acc: 50.91%\n",
      "\n",
      "----------------------------------------\n",
      "Epoch 57/500\n",
      "Train Loss: 0.5565, Train Acc: 72.09%\n",
      "Val Loss: 0.7539, Val Acc: 50.91%\n",
      "\n",
      "----------------------------------------\n",
      "Epoch 58/500\n",
      "Train Loss: 0.5560, Train Acc: 70.77%\n",
      "Val Loss: 0.7504, Val Acc: 51.05%\n",
      "\n",
      "----------------------------------------\n",
      "Epoch 59/500\n",
      "Train Loss: 0.5493, Train Acc: 71.95%\n",
      "Val Loss: 0.7538, Val Acc: 50.98%\n",
      "\n",
      "----------------------------------------\n",
      "Epoch 60/500\n",
      "Train Loss: 0.5600, Train Acc: 70.86%\n",
      "Val Loss: 0.7466, Val Acc: 51.33%\n",
      "\n",
      "----------------------------------------\n",
      "Epoch [60/500]\n",
      "Train Loss: 0.5600, Train Acc: 70.86%\n",
      "Val Loss: 0.7466, Val Acc: 51.33%\n",
      "\n",
      "Epoch 61/500\n",
      "Train Loss: 0.5619, Train Acc: 69.90%\n",
      "Val Loss: 0.7508, Val Acc: 50.91%\n",
      "\n",
      "----------------------------------------\n",
      "Epoch 62/500\n",
      "Train Loss: 0.5552, Train Acc: 70.61%\n",
      "Val Loss: 0.7548, Val Acc: 50.77%\n",
      "\n",
      "----------------------------------------\n",
      "Epoch 63/500\n",
      "Train Loss: 0.5544, Train Acc: 71.03%\n",
      "Val Loss: 0.7494, Val Acc: 50.91%\n",
      "\n",
      "----------------------------------------\n",
      "Epoch 64/500\n",
      "Train Loss: 0.5513, Train Acc: 71.90%\n",
      "Val Loss: 0.7569, Val Acc: 50.77%\n",
      "\n",
      "----------------------------------------\n",
      "Epoch 65/500\n",
      "Train Loss: 0.5560, Train Acc: 71.48%\n",
      "Val Loss: 0.7555, Val Acc: 50.91%\n",
      "\n",
      "----------------------------------------\n",
      "Epoch 66/500\n",
      "Train Loss: 0.5591, Train Acc: 70.24%\n",
      "Val Loss: 0.7530, Val Acc: 50.98%\n",
      "\n",
      "----------------------------------------\n",
      "Epoch 67/500\n",
      "Train Loss: 0.5558, Train Acc: 70.41%\n",
      "Val Loss: 0.7542, Val Acc: 50.98%\n",
      "\n",
      "----------------------------------------\n",
      "Epoch 68/500\n",
      "Train Loss: 0.5532, Train Acc: 71.81%\n",
      "Val Loss: 0.7535, Val Acc: 50.91%\n",
      "\n",
      "----------------------------------------\n",
      "Epoch 69/500\n",
      "Train Loss: 0.5557, Train Acc: 70.35%\n",
      "Val Loss: 0.7484, Val Acc: 51.19%\n",
      "\n",
      "----------------------------------------\n",
      "Epoch 70/500\n",
      "Train Loss: 0.5514, Train Acc: 70.92%\n",
      "Val Loss: 0.7518, Val Acc: 50.91%\n",
      "\n",
      "----------------------------------------\n",
      "Epoch [70/500]\n",
      "Train Loss: 0.5514, Train Acc: 70.92%\n",
      "Val Loss: 0.7518, Val Acc: 50.91%\n",
      "\n",
      "Epoch 71/500\n",
      "Train Loss: 0.5569, Train Acc: 71.14%\n",
      "Val Loss: 0.7489, Val Acc: 51.19%\n",
      "\n",
      "----------------------------------------\n",
      "Epoch 72/500\n",
      "Train Loss: 0.5564, Train Acc: 71.11%\n",
      "Val Loss: 0.7542, Val Acc: 50.84%\n",
      "\n",
      "----------------------------------------\n",
      "Epoch 73/500\n",
      "Train Loss: 0.5543, Train Acc: 71.17%\n",
      "Val Loss: 0.7549, Val Acc: 50.77%\n",
      "\n",
      "----------------------------------------\n",
      "Epoch 74/500\n",
      "Train Loss: 0.5483, Train Acc: 70.92%\n",
      "Val Loss: 0.7531, Val Acc: 50.91%\n",
      "\n",
      "----------------------------------------\n",
      "Epoch 75/500\n",
      "Train Loss: 0.5567, Train Acc: 70.86%\n",
      "Val Loss: 0.7534, Val Acc: 51.05%\n",
      "\n",
      "----------------------------------------\n",
      "Epoch 76/500\n",
      "Train Loss: 0.5410, Train Acc: 71.73%\n",
      "Val Loss: 0.7527, Val Acc: 50.70%\n",
      "\n",
      "----------------------------------------\n",
      "Epoch 77/500\n",
      "Train Loss: 0.5474, Train Acc: 71.45%\n",
      "Val Loss: 0.7502, Val Acc: 51.26%\n",
      "\n",
      "----------------------------------------\n",
      "Epoch 78/500\n",
      "Train Loss: 0.5565, Train Acc: 70.49%\n",
      "Val Loss: 0.7495, Val Acc: 51.12%\n",
      "\n",
      "----------------------------------------\n",
      "Epoch 79/500\n",
      "Train Loss: 0.5515, Train Acc: 71.14%\n",
      "Val Loss: 0.7552, Val Acc: 50.84%\n",
      "\n",
      "----------------------------------------\n",
      "Epoch 80/500\n",
      "Train Loss: 0.5534, Train Acc: 70.66%\n",
      "Val Loss: 0.7511, Val Acc: 51.12%\n",
      "\n",
      "----------------------------------------\n",
      "Epoch [80/500]\n",
      "Train Loss: 0.5534, Train Acc: 70.66%\n",
      "Val Loss: 0.7511, Val Acc: 51.12%\n",
      "\n",
      "Epoch 81/500\n",
      "Train Loss: 0.5481, Train Acc: 71.03%\n",
      "Val Loss: 0.7523, Val Acc: 50.98%\n",
      "\n",
      "----------------------------------------\n",
      "Epoch 82/500\n",
      "Train Loss: 0.5535, Train Acc: 71.39%\n",
      "Val Loss: 0.7517, Val Acc: 50.91%\n",
      "\n",
      "----------------------------------------\n",
      "Epoch 83/500\n",
      "Train Loss: 0.5528, Train Acc: 70.86%\n",
      "Val Loss: 0.7522, Val Acc: 50.98%\n",
      "\n",
      "----------------------------------------\n",
      "Epoch 84/500\n",
      "Train Loss: 0.5571, Train Acc: 70.13%\n",
      "Val Loss: 0.7513, Val Acc: 50.91%\n",
      "\n",
      "----------------------------------------\n",
      "Epoch 85/500\n",
      "Train Loss: 0.5401, Train Acc: 71.70%\n",
      "Val Loss: 0.7507, Val Acc: 50.84%\n",
      "\n",
      "----------------------------------------\n",
      "Epoch 86/500\n",
      "Train Loss: 0.5574, Train Acc: 71.31%\n",
      "Val Loss: 0.7527, Val Acc: 50.98%\n",
      "\n",
      "----------------------------------------\n",
      "Epoch 87/500\n",
      "Train Loss: 0.5577, Train Acc: 70.30%\n",
      "Val Loss: 0.7579, Val Acc: 51.05%\n",
      "\n",
      "----------------------------------------\n",
      "Epoch 88/500\n",
      "Train Loss: 0.5519, Train Acc: 70.66%\n",
      "Val Loss: 0.7500, Val Acc: 50.98%\n",
      "\n",
      "----------------------------------------\n",
      "Epoch 89/500\n",
      "Train Loss: 0.5458, Train Acc: 70.52%\n",
      "Val Loss: 0.7569, Val Acc: 50.91%\n",
      "\n",
      "----------------------------------------\n",
      "Epoch 90/500\n",
      "Train Loss: 0.5544, Train Acc: 70.92%\n",
      "Val Loss: 0.7562, Val Acc: 50.91%\n",
      "\n",
      "----------------------------------------\n",
      "Epoch [90/500]\n",
      "Train Loss: 0.5544, Train Acc: 70.92%\n",
      "Val Loss: 0.7562, Val Acc: 50.91%\n",
      "\n",
      "Epoch 91/500\n",
      "Train Loss: 0.5460, Train Acc: 71.36%\n",
      "Val Loss: 0.7524, Val Acc: 51.12%\n",
      "\n",
      "----------------------------------------\n",
      "Epoch 92/500\n",
      "Train Loss: 0.5547, Train Acc: 71.48%\n",
      "Val Loss: 0.7510, Val Acc: 51.19%\n",
      "\n",
      "----------------------------------------\n",
      "Epoch 93/500\n",
      "Train Loss: 0.5529, Train Acc: 71.65%\n",
      "Val Loss: 0.7531, Val Acc: 50.98%\n",
      "\n",
      "----------------------------------------\n",
      "Epoch 94/500\n",
      "Train Loss: 0.5497, Train Acc: 71.90%\n",
      "Val Loss: 0.7479, Val Acc: 51.33%\n",
      "\n",
      "----------------------------------------\n",
      "Epoch 95/500\n",
      "Train Loss: 0.5417, Train Acc: 72.09%\n",
      "Val Loss: 0.7554, Val Acc: 50.84%\n",
      "\n",
      "----------------------------------------\n",
      "Epoch 96/500\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[20], line 16\u001b[0m\n\u001b[0;32m     12\u001b[0m global_history \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain_loss\u001b[39m\u001b[38;5;124m'\u001b[39m: [], \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mval_loss\u001b[39m\u001b[38;5;124m'\u001b[39m: [], \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain_acc\u001b[39m\u001b[38;5;124m'\u001b[39m: [], \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mval_acc\u001b[39m\u001b[38;5;124m'\u001b[39m: []}\n\u001b[0;32m     15\u001b[0m model \u001b[38;5;241m=\u001b[39m TCC(input_features\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m40\u001b[39m, sequence_length\u001b[38;5;241m=\u001b[39mSEQMENT_LENGTH) \n\u001b[1;32m---> 16\u001b[0m model, history \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m500\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.0001\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mglobal_history\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mglobal_history\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpatience\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m500\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[19], line 216\u001b[0m, in \u001b[0;36mtrain_model\u001b[1;34m(model, train_loader, val_loader, num_epochs, learning_rate, global_history, patience)\u001b[0m\n\u001b[0;32m    213\u001b[0m torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mclip_grad_norm_(model\u001b[38;5;241m.\u001b[39mparameters(), max_norm\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1.0\u001b[39m)\n\u001b[0;32m    214\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m--> 216\u001b[0m train_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    217\u001b[0m _, predicted \u001b[38;5;241m=\u001b[39m outputs\u001b[38;5;241m.\u001b[39mmax(\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m    218\u001b[0m train_total \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m labels\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "BATCH_SIZE = 32\n",
    "SEQMENT_LENGTH = 200\n",
    "\n",
    "train_loader = DataLoader(dataset_train, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_loader = DataLoader(dataset_val, batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "# set for reduced memory usage\n",
    "\n",
    "\n",
    "# torch.set_default_tensor_type(torch.cuda.FloatTensor)\n",
    "\n",
    "global_history = {'train_loss': [], 'val_loss': [], 'train_acc': [], 'val_acc': []}\n",
    "\n",
    "\n",
    "model = TCC(input_features=40, sequence_length=SEQMENT_LENGTH) \n",
    "model, history = train_model(model, train_loader, val_loader, num_epochs=500, learning_rate=0.0001, global_history=global_history, patience=500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# open history_TCC_inside.pkl'\n",
    "import pickle\n",
    "with open('history_TCC_inside_balanced_FINAL.pkl', 'rb') as f:\n",
    "    history = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TCC(\n",
       "  (cnn_stream1): CNNStream(\n",
       "    (layers): Sequential(\n",
       "      (0): Conv1d(40, 16, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "      (1): ReLU()\n",
       "      (2): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "      (3): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (4): Dropout(p=0.5, inplace=False)\n",
       "      (5): Conv1d(16, 32, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "      (6): ReLU()\n",
       "      (7): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "      (8): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (9): Dropout(p=0.5, inplace=False)\n",
       "      (10): Conv1d(32, 64, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "      (11): ReLU()\n",
       "      (12): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "      (13): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (14): Dropout(p=0.5, inplace=False)\n",
       "    )\n",
       "  )\n",
       "  (cnn_stream2): CNNStream(\n",
       "    (layers): Sequential(\n",
       "      (0): Conv1d(40, 16, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "      (1): ReLU()\n",
       "      (2): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "      (3): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (4): Dropout(p=0.5, inplace=False)\n",
       "      (5): Conv1d(16, 32, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "      (6): ReLU()\n",
       "      (7): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "      (8): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (9): Dropout(p=0.5, inplace=False)\n",
       "      (10): Conv1d(32, 64, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "      (11): ReLU()\n",
       "      (12): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "      (13): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (14): Dropout(p=0.5, inplace=False)\n",
       "    )\n",
       "  )\n",
       "  (transformer_stream): TransformerStream(\n",
       "    (input_proj): Linear(in_features=40, out_features=128, bias=True)\n",
       "    (transformer_layers): ModuleList(\n",
       "      (0-3): 4 x TransformerLayer(\n",
       "        (attention): LinearAttention(\n",
       "          (q_proj): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (k_proj): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (v_proj): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (o_proj): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (elu): ELU(alpha=1.0)\n",
       "        )\n",
       "        (ffn): Sequential(\n",
       "          (0): Linear(in_features=128, out_features=1024, bias=True)\n",
       "          (1): ReLU()\n",
       "          (2): Dropout(p=0.4, inplace=False)\n",
       "          (3): Linear(in_features=1024, out_features=128, bias=True)\n",
       "        )\n",
       "        (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "    (pool): AdaptiveAvgPool1d(output_size=1)\n",
       "  )\n",
       "  (fusion): Sequential(\n",
       "    (0): Linear(in_features=3328, out_features=128, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Dropout(p=0.4, inplace=False)\n",
       "    (3): Linear(in_features=128, out_features=2, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### load the saved model and make predictions best_model.pth\n",
    "SEQMENT_LENGTH = 200\n",
    "BATCH_SIZE = 32\n",
    "model = TCC(input_features=40, sequence_length=SEQMENT_LENGTH) \n",
    "model.load_state_dict(torch.load('best_model_TCC_inside_balanced_FINAL.pth'))\n",
    "model.eval()  # Set the model to evaluation mode"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load dataset\n",
    "\n",
    "Train is splitted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before balancing - Train: [4378 1781]\n",
      "After balancing - Train: [1781 1781]\n",
      "Before balancing - Test: [967 712]\n",
      "After balancing - Test: [712 712]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.utils import shuffle\n",
    "\n",
    "\n",
    "X_train, y_train = load_mfcc('DAIC_train.npz')\n",
    "X_test, y_test = load_mfcc('DAIC_test.npz')\n",
    "\n",
    "# shuffle the data\n",
    "X_train, y_train = shuffle(X_train, y_train, random_state=33)\n",
    "X_test, y_test = shuffle(X_test, y_test, random_state=33)\n",
    "\n",
    "# Flatten the MFCC data from 3D to 2D for resampling\n",
    "X_train_flattened = X_train.reshape(X_train.shape[0], -1)\n",
    "X_test_flattened = X_test.reshape(X_test.shape[0], -1)\n",
    "\n",
    "# Initialize the RandomUnderSampler\n",
    "rus = RandomUnderSampler(random_state=42)\n",
    "\n",
    "# Resample the dataset\n",
    "X_train_resampled, y_train_resampled = rus.fit_resample(X_train_flattened, y_train)\n",
    "X_test_resampled, y_test_resampled = rus.fit_resample(X_test_flattened, y_test)\n",
    "\n",
    "# Check the balance\n",
    "print(\"Before balancing - Train:\", np.bincount(y_train))\n",
    "print(\"After balancing - Train:\", np.bincount(y_train_resampled))\n",
    "print(\"Before balancing - Test:\", np.bincount(y_test))\n",
    "print(\"After balancing - Test:\", np.bincount(y_test_resampled))\n",
    "\n",
    "# Optional: Reshape back to 3D if your model requires\n",
    "X_train_resampled_3d = X_train_resampled.reshape(-1, 40, 200)  # Modify dimensions as necessary\n",
    "X_test_resampled_3d = X_test_resampled.reshape(-1, 40, 200)  # Modify dimensions as necessary\n",
    "\n",
    "dataset_train = AudioDataset(X_train_resampled_3d, y_train_resampled)\n",
    "dataset_val = AudioDataset(X_test_resampled_3d, y_test_resampled)\n",
    "\n",
    "train_loader = DataLoader(dataset_train, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_loader = DataLoader(dataset_val, batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# unbalanced validation \n",
    "# I dont have time for this. \n",
    "\n",
    "# X_train_resampled_3d, X_val_resampled_3d, y_train_resampled, y_val_resampled = train_test_split(X_train_resampled_3d, y_train_resampled, test_size=0.2, random_state=42)\n",
    "\n",
    "# dataset_val = AudioDataset(X_test, y_test)\n",
    "# val_loader = DataLoader(dataset_val, batch_size=BATCH_SIZE, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Training Confusion Matrix:\n",
      " [[ 296 1485]\n",
      " [  15 1766]]\n",
      "Validation Confusion Matrix:\n",
      " [[ 61 651]\n",
      " [ 44 668]]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# Check if CUDA is available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)\n",
    "\n",
    "# Move model to the appropriate device\n",
    "model.to(device)\n",
    "\n",
    "def get_all_preds(model, loader):\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    with torch.no_grad():\n",
    "        for data, labels in loader:\n",
    "            # Move data to the same device as the model\n",
    "            data = data.to(device)\n",
    "            labels = labels.to(device)\n",
    "            \n",
    "            outputs = model(data)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "    return all_labels, all_preds\n",
    "\n",
    "# Assuming train_loader and val_loader are defined as DataLoader instances\n",
    "# Get predictions for training data\n",
    "train_labels, train_preds = get_all_preds(model, train_loader)\n",
    "train_conf_matrix = confusion_matrix(train_labels, train_preds)\n",
    "\n",
    "# Get predictions for validation data\n",
    "val_labels, val_preds = get_all_preds(model, val_loader)\n",
    "val_conf_matrix = confusion_matrix(val_labels, val_preds)\n",
    "\n",
    "print(\"Training Confusion Matrix:\\n\", train_conf_matrix)\n",
    "print(\"Validation Confusion Matrix:\\n\", val_conf_matrix)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 500x200 with 4 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import importlib as imp\n",
    "\n",
    "\n",
    "import functions_data_viz\n",
    "imp.reload(functions_data_viz)\n",
    "from functions_data_viz import plot_precomputed_conf_matrices\n",
    "\n",
    "# Example usage with your matrices\n",
    "plot_precomputed_conf_matrices(train_conf_matrix, val_conf_matrix, show_plot=\"all\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
