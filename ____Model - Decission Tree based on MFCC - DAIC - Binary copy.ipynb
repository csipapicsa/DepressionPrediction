{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "#dev\n",
    "import importlib as imp\n",
    "import functions\n",
    "from functions import *\n",
    "import functions_data_prep\n",
    "from functions_data_prep import *\n",
    "\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_PATH = \"datasets/DAIC-WOZ/\"\n",
    "files = os.listdir(BASE_PATH+\"/cuts\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read in pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(BASE_PATH + \"Patient_Classes.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Participant_ID</th>\n",
       "      <th>PHQ8_Binary</th>\n",
       "      <th>PHQ8_Score</th>\n",
       "      <th>PHQ8_multiclass</th>\n",
       "      <th>dataset</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>303</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>304</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>305</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>310</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>312</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>137</th>\n",
       "      <td>483</td>\n",
       "      <td>1</td>\n",
       "      <td>15</td>\n",
       "      <td>3</td>\n",
       "      <td>dev</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>138</th>\n",
       "      <td>484</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>dev</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>139</th>\n",
       "      <td>489</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>dev</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>140</th>\n",
       "      <td>490</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>dev</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>141</th>\n",
       "      <td>492</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>dev</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>142 rows Ã— 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Participant_ID  PHQ8_Binary  PHQ8_Score  PHQ8_multiclass dataset\n",
       "0               303            0           0                0   train\n",
       "1               304            0           6                1   train\n",
       "2               305            0           7                1   train\n",
       "3               310            0           4                0   train\n",
       "4               312            0           2                0   train\n",
       "..              ...          ...         ...              ...     ...\n",
       "137             483            1          15                3     dev\n",
       "138             484            0           9                1     dev\n",
       "139             489            0           3                0     dev\n",
       "140             490            0           2                0     dev\n",
       "141             492            0           0                0     dev\n",
       "\n",
       "[142 rows x 5 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SAve AVG stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "imp.reload(functions_data_prep)\n",
    "from functions_data_prep import *\n",
    "\n",
    "BASE_PATH_CUTS = \"datasets/DAIC-WOZ/cuts/\"\n",
    "PATIENT = 419\n",
    "\n",
    "MODE = \"Binary\" # or Multiclass\n",
    "\n",
    "# Path to the .npy file\n",
    "feature_path = BASE_PATH_CUTS+f\"best_paper_features_only_mfcc_{str(PATIENT)}.npy\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imp.reload(functions_data_prep)\n",
    "from functions_data_prep import *\n",
    "\n",
    "# Usage\n",
    "\n",
    "organized_features = load_and_organize_features(feature_path)\n",
    "\n",
    "# Now you can access features for each chunk like this:\n",
    "# first_chunk_features = organized_features[0]\n",
    "#mfcc_means = first_chunk_features['mfcc_mean']\n",
    "#pitch = first_chunk_features['pitch']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_feature_summary_df(organized_features):\n",
    "    \"\"\"\n",
    "    Create a DataFrame with min, max, and avg values for all features across chunks.\n",
    "    \"\"\"\n",
    "    # First collect all values for each feature\n",
    "    feature_values = {\n",
    "        'mfcc_mean': np.array([chunk['mfcc_mean'] for chunk in organized_features]),\n",
    "        'mfcc_std': np.array([chunk['mfcc_std'] for chunk in organized_features]),\n",
    "        'spectral_centroid': np.array([chunk['spectral_centroid_mean'] for chunk in organized_features]),\n",
    "        'spectral_bandwidth': np.array([chunk['spectral_bandwidth_mean'] for chunk in organized_features]),\n",
    "        'spectral_rolloff': np.array([chunk['spectral_rolloff_mean'] for chunk in organized_features]),\n",
    "        'zero_crossing_rate': np.array([chunk['zero_crossing_rate_mean'] for chunk in organized_features]),\n",
    "        'pitch': np.array([chunk['pitch'] for chunk in organized_features]),\n",
    "        'energy': np.array([chunk['energy'] for chunk in organized_features]),\n",
    "        'speech_rate': np.array([chunk['speech_rate'] for chunk in organized_features])\n",
    "    }\n",
    "    \n",
    "    # Create a dictionary to store all features\n",
    "    all_features = {}\n",
    "    \n",
    "    # Handle MFCC coefficients (40 of them)\n",
    "    for i in range(40):\n",
    "        # For MFCC means\n",
    "        all_features[f'mfcc_mean_{i}_min'] = np.min(feature_values['mfcc_mean'][:, i])\n",
    "        all_features[f'mfcc_mean_{i}_max'] = np.max(feature_values['mfcc_mean'][:, i])\n",
    "        all_features[f'mfcc_mean_{i}_avg'] = np.mean(feature_values['mfcc_mean'][:, i])\n",
    "        \n",
    "        # For MFCC standard deviations\n",
    "        all_features[f'mfcc_std_{i}_min'] = np.min(feature_values['mfcc_std'][:, i])\n",
    "        all_features[f'mfcc_std_{i}_max'] = np.max(feature_values['mfcc_std'][:, i])\n",
    "        all_features[f'mfcc_std_{i}_avg'] = np.mean(feature_values['mfcc_std'][:, i])\n",
    "    \n",
    "    # Handle other features\n",
    "    for feature in ['spectral_centroid', 'spectral_bandwidth', 'spectral_rolloff', \n",
    "                   'zero_crossing_rate', 'pitch', 'energy', 'speech_rate']:\n",
    "        all_features[f'{feature}_min'] = np.min(feature_values[feature])\n",
    "        all_features[f'{feature}_max'] = np.max(feature_values[feature])\n",
    "        all_features[f'{feature}_avg'] = np.mean(feature_values[feature])\n",
    "    \n",
    "    # Create DataFrame with a single row\n",
    "    df = pd.DataFrame([all_features])\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error with patient 491\n",
      "Error with patient 489\n",
      "Error with patient 490\n",
      "Error with patient 492\n"
     ]
    }
   ],
   "source": [
    "for _, row in df.iterrows():\n",
    "    patient_id =  row['Participant_ID']\n",
    "    try:\n",
    "        feature_path = BASE_PATH_CUTS+f\"best_paper_features_only_mfcc_{str(patient_id)}.npy\"\n",
    "        organized_features = load_and_organize_features(feature_path)\n",
    "\n",
    "        create_feature_summary_df(organized_features).to_csv(f\"{BASE_PATH_CUTS}_AVG_MIN_MAX_{patient_id}.csv\")\n",
    "    except:\n",
    "        print(f\"Error with patient {patient_id}\")\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import mode  # Option 1: Using scipy\n",
    "\n",
    "def aggregate_patient_chunks(features_list, labels, chunk_size=10):\n",
    "    \"\"\"\n",
    "    Aggregate features from multiple chunks for each patient.\n",
    "    \n",
    "    Args:\n",
    "        features_list: List of feature dictionaries\n",
    "        labels: Corresponding labels\n",
    "        chunk_size: Number of chunks to aggregate\n",
    "    \"\"\"\n",
    "    n_chunks = len(features_list)\n",
    "    n_groups = n_chunks // chunk_size\n",
    "    \n",
    "    aggregated_features = []\n",
    "    aggregated_labels = []\n",
    "    \n",
    "    for i in range(n_groups):\n",
    "        start_idx = i * chunk_size\n",
    "        end_idx = start_idx + chunk_size\n",
    "        chunk_group = features_list[start_idx:end_idx]\n",
    "        \n",
    "        # Aggregate MFCC features\n",
    "        mfcc_means = np.array([chunk['mfcc_mean'] for chunk in chunk_group])\n",
    "        mfcc_stds = np.array([chunk['mfcc_std'] for chunk in chunk_group])\n",
    "        \n",
    "        # Calculate statistics over the chunks\n",
    "        group_features = {\n",
    "            # Mean and std of MFCC means across chunks\n",
    "            'mfcc_mean_mean': np.mean(mfcc_means, axis=0),\n",
    "            'mfcc_mean_std': np.std(mfcc_means, axis=0),\n",
    "            # Mean and std of MFCC stds across chunks\n",
    "            'mfcc_std_mean': np.mean(mfcc_stds, axis=0),\n",
    "            'mfcc_std_std': np.std(mfcc_stds, axis=0),\n",
    "            # Other features averaged across chunks\n",
    "            'spectral_centroid_mean': np.mean([chunk['spectral_centroid_mean'] for chunk in chunk_group]),\n",
    "            'spectral_bandwidth_mean': np.mean([chunk['spectral_bandwidth_mean'] for chunk in chunk_group]),\n",
    "            'spectral_rolloff_mean': np.mean([chunk['spectral_rolloff_mean'] for chunk in chunk_group]),\n",
    "            'zero_crossing_rate_mean': np.mean([chunk['zero_crossing_rate_mean'] for chunk in chunk_group]),\n",
    "            'pitch_mean': np.mean([chunk['pitch'] for chunk in chunk_group]),\n",
    "            'energy_mean': np.mean([chunk['energy'] for chunk in chunk_group]),\n",
    "            'speech_rate_mean': np.mean([chunk['speech_rate'] for chunk in chunk_group])\n",
    "        }\n",
    "        \n",
    "        # Flatten the features into a single vector\n",
    "        feature_vector = np.concatenate([\n",
    "            group_features['mfcc_mean_mean'],\n",
    "            group_features['mfcc_mean_std'],\n",
    "            group_features['mfcc_std_mean'],\n",
    "            group_features['mfcc_std_std'],\n",
    "            [group_features['spectral_centroid_mean']],\n",
    "            [group_features['spectral_bandwidth_mean']],\n",
    "            [group_features['spectral_rolloff_mean']],\n",
    "            [group_features['zero_crossing_rate_mean']],\n",
    "            [group_features['pitch_mean']],\n",
    "            [group_features['energy_mean']],\n",
    "            [group_features['speech_rate_mean']]\n",
    "        ])\n",
    "        \n",
    "        aggregated_features.append(feature_vector)\n",
    "        # Take the most common label in the chunk group\n",
    "        aggregated_labels.append(mode(labels[start_idx:end_idx]))\n",
    "    \n",
    "    return np.array(aggregated_features), np.array(aggregated_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_aggregated, y_aggregated = aggregate_patient_chunks(organized_features, labels, chunk_size=5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[9, 5],\n",
       "       [9, 5],\n",
       "       [9, 5],\n",
       "       [9, 5],\n",
       "       [9, 5],\n",
       "       [9, 5],\n",
       "       [9, 5],\n",
       "       [9, 5],\n",
       "       [9, 5]], dtype=int64)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_aggregated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_decision_tree(features, labels):\n",
    "    \"\"\"Train and evaluate a decision tree classifier\"\"\"\n",
    "    # Create and train the decision tree\n",
    "    clf = DecisionTreeClassifier(random_state=42)\n",
    "    clf.fit(features, labels)\n",
    "    return clf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aggregate features\n",
    "X_aggregated, y_aggregated = aggregate_patient_chunks(organized_features, labels, chunk_size=10)\n",
    "\n",
    "# Split into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_aggregated, y_aggregated, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train model\n",
    "clf = train_decision_tree(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "# Print results\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(confusion_matrix(y_test, y_pred))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
