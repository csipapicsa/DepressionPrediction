\section{Introduction}

Depression affects millions globally and represents a significant public health issue \cite{WHO_Mental_Disorders_2019}. Early detection and intervention are critical for effective management and treatment. Traditionally, depression assessment has relied heavily on clinical interviews and self-reported measures, such as the PHQ-8 \cite{phq8} and PHQ-9 questionnaire \cite{kroenke2001phq}. 

This research aims to advance the field of psychiatric diagnostics by exploring the potential of audio analysis to detect depression. Utilizing machine learning algorithms, specifically Decision Tree and Convolutional Neural Network (CNN) models, this study analyzes vocal biomarkers within audio recordings from two distinct datasets: the Distress Analysis Interview Corpus - Wizard of Oz (DAIC) \cite{DAICWOZ} and the Emotional Audio-Textual Depression (EATD) Corpus \cite{shen2022automaticdepressiondetectionemotional}. These datasets offer source of vocal expressions aligned with validated depression assessments, providing a foundation for developing predictive models.

The challenge of accurately detecting depression from audio features encompasses several critical issues. These include addressing the class imbalance across different depression severity categories, managing the variability in audio quality, and ensuring the generalizability of models beyond the training data. This study aims to analyze these relationships between audio characteristics and depression and explore the viability of audio-based depression detection as a supplementary tool to traditional methods.

Despite the initial aim of utilizing machine learning to enhance the diagnosis of depression through audio analysis, this study reveals significant methodological challenges. A fundamental limitation is that current approaches attempt to replicate questionnaire-based assessments (PHQ-8/9) through complex machine learning models, rather than discovering novel audio biomarkers that might provide additional clinical insights. Furthermore, this research underscores a critical flaw in many studies: using the same patients' audio samples for both training and testing, which, while improving model accuracy, fails to ensure generalizability to new individuals. These insights highlight the importance of methodological rigor and the need to prioritize practical generalization in future machine learning research. Additionally, the choice and processing of audio features play a crucial role, proving essential for the effective performance and reliability of machine learning models in clinical applications.