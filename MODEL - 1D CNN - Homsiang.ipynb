{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Homsiang - MFCC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 1: Define Helper Functions for Segmenting and Padding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "### load data and labels\n",
    "### mfcc has various lengths \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def segment_mfcc(mfcc, max_segment_length=200):\n",
    "    # Segment the MFCC array into fixed lengths with possible overlap (if desired)\n",
    "    # segment_length is the fixed length of each segment\n",
    "    segments = []\n",
    "    for start in range(0, mfcc.shape[1], max_segment_length):\n",
    "        end = start + max_segment_length\n",
    "        if end < mfcc.shape[1]:\n",
    "            segments.append(mfcc[:, start:end])\n",
    "        else:\n",
    "            # Padding the last segment if it's shorter than the required segment length\n",
    "            segments.append(np.pad(mfcc[:, start:], ((0,0), (0, max_segment_length - (mfcc.shape[1] - start))), 'constant'))\n",
    "    return segments\n",
    "\n",
    "def load_concatenated_mfcc(path):\n",
    "    \n",
    "    # Load the concatenated MFCC data\n",
    "    data = np.load(path)\n",
    "    return data['mfcc']\n",
    "\n",
    "df = pd.read_csv('datasets/DAIC-WOZ/Patient_Classes.csv')\n",
    "\n",
    "# for _, row in df.iterrows():\n",
    "#     patient_id = row['Participant_ID']\n",
    "#     label = row['PHQ8_Binary']\n",
    "#     train_or_test = row['dataset'] # test or dev\n",
    "#     mfcc_path = 'datasets/DAIC-WOZ/ConcatenatedMFCC/concatenated_mfcc_'+str(patient_id)+'.npz'\n",
    "#     try:\n",
    "#         mfcc = load_concatenated_mfcc(mfcc_path) # the raw mfcc data\n",
    "#         # make a dataset to be able to train the CNN\n",
    "\n",
    "#     except Exception as e:\n",
    "#         print(e)\n",
    "#         continue\n",
    "\n",
    "def create_datasets(df, max_segment_length=500):\n",
    "    dataset = {'train': [], 'test': []}\n",
    "    labels = {'train': [], 'test': []}\n",
    "\n",
    "    for _, row in df.iterrows():\n",
    "        patient_id = row['Participant_ID']\n",
    "        label = row['PHQ8_Binary']\n",
    "        print(patient_id, label)\n",
    "        train_or_test = row['dataset']  # Could be 'train' or 'test'\n",
    "        # if train_or_test is 'dev' change it to 'test'\n",
    "        if train_or_test == 'dev':\n",
    "            train_or_test = 'test'\n",
    "\n",
    "        mfcc_path = f'datasets/DAIC-WOZ/ConcatenatedMFCC/concatenated_mfcc_{patient_id}.npz'\n",
    "\n",
    "        try:\n",
    "            mfcc = load_concatenated_mfcc(mfcc_path)  # Load the raw MFCC data\n",
    "            segments = segment_mfcc(mfcc, max_segment_length=max_segment_length)\n",
    "            \n",
    "            # Append each segment to the corresponding dataset\n",
    "            for segment in segments:\n",
    "                dataset[train_or_test].append(segment)\n",
    "                labels[train_or_test].append(label)\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to process patient {patient_id}: {e}\")\n",
    "\n",
    "    return dataset, labels\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 2: Initialize DataLoaders\n",
    "python\n",
    "Copy code\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "303 0\n",
      "304 0\n",
      "305 0\n",
      "310 0\n",
      "312 0\n",
      "313 0\n",
      "315 0\n",
      "316 0\n",
      "317 0\n",
      "318 0\n",
      "319 1\n",
      "320 1\n",
      "321 1\n",
      "322 0\n",
      "324 0\n",
      "325 1\n",
      "326 0\n",
      "327 0\n",
      "328 0\n",
      "330 1\n",
      "333 0\n",
      "336 0\n",
      "338 1\n",
      "339 1\n",
      "340 0\n",
      "341 0\n",
      "343 0\n",
      "344 1\n",
      "345 1\n",
      "347 1\n",
      "348 1\n",
      "350 1\n",
      "351 1\n",
      "352 1\n",
      "353 1\n",
      "355 1\n",
      "356 1\n",
      "357 0\n",
      "358 0\n",
      "360 0\n",
      "362 1\n",
      "363 0\n",
      "364 0\n",
      "366 0\n",
      "368 0\n",
      "369 0\n",
      "370 0\n",
      "371 0\n",
      "372 1\n",
      "374 0\n",
      "375 0\n",
      "376 1\n",
      "379 0\n",
      "380 1\n",
      "383 0\n",
      "385 0\n",
      "386 1\n",
      "391 0\n",
      "392 0\n",
      "393 0\n",
      "397 0\n",
      "400 0\n",
      "401 0\n",
      "402 1\n",
      "409 0\n",
      "412 1\n",
      "414 1\n",
      "415 0\n",
      "416 0\n",
      "419 0\n",
      "423 0\n",
      "425 0\n",
      "426 1\n",
      "427 0\n",
      "428 0\n",
      "429 0\n",
      "430 0\n",
      "433 1\n",
      "434 0\n",
      "437 0\n",
      "441 1\n",
      "443 0\n",
      "444 0\n",
      "445 0\n",
      "446 0\n",
      "447 0\n",
      "448 1\n",
      "449 0\n",
      "454 0\n",
      "455 0\n",
      "456 0\n",
      "457 0\n",
      "459 1\n",
      "463 0\n",
      "464 0\n",
      "468 0\n",
      "471 0\n",
      "473 0\n",
      "474 0\n",
      "475 0\n",
      "478 0\n",
      "479 0\n",
      "485 0\n",
      "486 0\n",
      "487 0\n",
      "488 0\n",
      "491 0\n",
      "Failed to process patient 491: [Errno 2] No such file or directory: 'datasets/DAIC-WOZ/ConcatenatedMFCC/concatenated_mfcc_491.npz'\n",
      "302 0\n",
      "307 0\n",
      "331 0\n",
      "335 1\n",
      "346 1\n",
      "367 1\n",
      "377 1\n",
      "381 1\n",
      "382 0\n",
      "388 1\n",
      "389 1\n",
      "390 0\n",
      "395 0\n",
      "403 0\n",
      "404 0\n",
      "406 0\n",
      "413 1\n",
      "417 0\n",
      "418 1\n",
      "420 0\n",
      "422 1\n",
      "436 0\n",
      "439 0\n",
      "440 1\n",
      "451 0\n",
      "458 0\n",
      "472 0\n",
      "476 0\n",
      "477 0\n",
      "482 0\n",
      "483 1\n",
      "484 0\n",
      "489 0\n",
      "Failed to process patient 489: [Errno 2] No such file or directory: 'datasets/DAIC-WOZ/ConcatenatedMFCC/concatenated_mfcc_489.npz'\n",
      "490 0\n",
      "Failed to process patient 490: [Errno 2] No such file or directory: 'datasets/DAIC-WOZ/ConcatenatedMFCC/concatenated_mfcc_490.npz'\n",
      "492 0\n",
      "Failed to process patient 492: [Errno 2] No such file or directory: 'datasets/DAIC-WOZ/ConcatenatedMFCC/concatenated_mfcc_492.npz'\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import TensorDataset, DataLoader, Dataset\n",
    "\n",
    "def prepare_dataloaders(dataset, labels, batch_size=32):\n",
    "    dataloaders = {}\n",
    "    for phase in ['train', 'test']:\n",
    "        features = torch.tensor(dataset[phase]).float()  # Convert features to float tensors\n",
    "        targets = torch.tensor(labels[phase]).long()  # Convert labels to long tensors\n",
    "\n",
    "        # Reshape for Conv1D: [batch, channels, length]\n",
    "        features = features.permute(0, 1, 2)\n",
    "\n",
    "        data_set = TensorDataset(features, targets)\n",
    "        dataloaders[phase] = DataLoader(data_set, batch_size=batch_size, shuffle=(phase == 'train'))\n",
    "    \n",
    "    return dataloaders\n",
    "\n",
    "SEQMENT_LENGTH = 50\n",
    "# Assuming you have already loaded and segmented the data\n",
    "dataset, labels = create_datasets(df, max_segment_length=SEQMENT_LENGTH) \n",
    "# dataloaders = prepare_dataloaders(dataset, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AudioDataset(Dataset):\n",
    "    def __init__(self, mfcc_features, labels):\n",
    "        self.features = torch.FloatTensor(mfcc_features)\n",
    "        self.labels = torch.LongTensor(labels)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        return self.features[idx], self.labels[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Gergo Gyori\\AppData\\Local\\Temp\\ipykernel_17580\\587900977.py:3: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ..\\torch\\csrc\\utils\\tensor_new.cpp:277.)\n",
      "  self.features = torch.FloatTensor(mfcc_features)\n"
     ]
    }
   ],
   "source": [
    "dataset_train = AudioDataset(dataset[\"train\"], labels[\"test\"])\n",
    "dataset_val = AudioDataset(dataset[\"test\"], labels[\"test\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class DepressionDetector1DCNN(nn.Module):\n",
    "    def __init__(self, input_features=40, sequence_length=200):\n",
    "        super(DepressionDetector1DCNN, self).__init__()\n",
    "        \n",
    "        self.conv_layers = nn.Sequential(\n",
    "            nn.Conv1d(input_features, 64, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool1d(2),\n",
    "            nn.BatchNorm1d(64),\n",
    "            \n",
    "            nn.Conv1d(64, 128, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool1d(2),\n",
    "            nn.BatchNorm1d(128),\n",
    "            \n",
    "            nn.Conv1d(128, 256, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool1d(2),\n",
    "            nn.BatchNorm1d(256)\n",
    "        )\n",
    "        \n",
    "        self.flatten = nn.Flatten()\n",
    "        \n",
    "        # Calculate the size after convolutions\n",
    "        # After each MaxPool1d(2), the sequence length is halved\n",
    "        final_sequence_length = sequence_length // (2 * 2 * 2)  # Three MaxPool layers\n",
    "        self.dense_input_size = 256 * final_sequence_length\n",
    "        \n",
    "        self.dense_layers = nn.Sequential(\n",
    "            nn.Linear(self.dense_input_size, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.7), # add more\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5), # add more\n",
    "            nn.Linear(64, 2)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Input shape: [batch_size, features, sequence_length]\n",
    "        x = self.conv_layers(x)\n",
    "        x = self.flatten(x)\n",
    "        x = self.dense_layers(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(train_loader, val_loader, sequence_length=200, num_epochs=200, batch_size=16, learning_rate=0.001):\n",
    "    # Initialize model, loss function, and optimizer\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model = DepressionDetector1DCNN(input_features=40, sequence_length=sequence_length).to(device)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    # optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate, weight_decay=0.01)\n",
    "    \n",
    "    # Training loop\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        train_correct = 0\n",
    "        train_total = 0\n",
    "        \n",
    "        for features, labels in train_loader:\n",
    "            features, labels = features.to(device), labels.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(features)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            train_loss += loss.item()\n",
    "            _, predicted = outputs.max(1)\n",
    "            train_total += labels.size(0)\n",
    "            train_correct += predicted.eq(labels).sum().item()\n",
    "            \n",
    "        # Validation\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        val_correct = 0\n",
    "        val_total = 0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for features, labels in val_loader:\n",
    "                features, labels = features.to(device), labels.to(device)\n",
    "                outputs = model(features)\n",
    "                loss = criterion(outputs, labels)\n",
    "                \n",
    "                val_loss += loss.item()\n",
    "                _, predicted = outputs.max(1)\n",
    "                val_total += labels.size(0)\n",
    "                val_correct += predicted.eq(labels).sum().item()\n",
    "        \n",
    "        # Print epoch statistics\n",
    "        if (epoch + 1) % 10 == 0:\n",
    "            print(f'Epoch [{epoch+1}/{num_epochs}]')\n",
    "            print(f'Train Loss: {train_loss/len(train_loader):.4f}, '\n",
    "                  f'Train Acc: {100.*train_correct/train_total:.2f}%')\n",
    "            print(f'Val Loss: {val_loss/len(val_loader):.4f}, '\n",
    "                  f'Val Acc: {100.*val_correct/val_total:.2f}%\\n')\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# another CNN\n",
    "class DepressionDetectorCNN(nn.Module):\n",
    "    def __init__(self, input_features=40, sequence_length=200):\n",
    "        super(DepressionDetectorCNN, self).__init__()\n",
    "        \n",
    "        # First conv block - keeping input channels smaller than paper\n",
    "        self.conv1 = nn.Sequential(\n",
    "            nn.Conv1d(input_features, 32, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool1d(2),\n",
    "            nn.BatchNorm1d(32),\n",
    "            nn.Dropout(0.2)  # Adding dropout after each block\n",
    "        )\n",
    "        \n",
    "        # Second conv block\n",
    "        self.conv2 = nn.Sequential(\n",
    "            nn.Conv1d(32, 64, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool1d(2),\n",
    "            nn.BatchNorm1d(64),\n",
    "            nn.Dropout(0.3)\n",
    "        )\n",
    "        \n",
    "        # Third conv block\n",
    "        self.conv3 = nn.Sequential(\n",
    "            nn.Conv1d(64, 128, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool1d(2),\n",
    "            nn.BatchNorm1d(128),\n",
    "            nn.Dropout(0.4)\n",
    "        )\n",
    "        \n",
    "        # Calculate the size after convolutions\n",
    "        final_sequence_length = sequence_length // (2 * 2 * 2)  # Three MaxPool layers\n",
    "        self.dense_input_size = 128 * final_sequence_length\n",
    "        \n",
    "        # Dense layers with increased dropout\n",
    "        self.dense = nn.Sequential(\n",
    "            nn.Linear(self.dense_input_size, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(64, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(32, 2)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Input shape: [batch_size, features, sequence_length]\n",
    "        x = self.conv1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.conv3(x)\n",
    "        x = x.flatten(1)\n",
    "        x = self.dense(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Another full motherfucker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "\n",
    "class DepressionGCNN(nn.Module):\n",
    "    def __init__(self, input_features=40, sequence_length=200, num_neighbors=9):\n",
    "        super(DepressionGCNN, self).__init__()\n",
    "        \n",
    "        self.sequence_length = sequence_length\n",
    "        self.num_neighbors = num_neighbors\n",
    "        \n",
    "        # Graph convolutional layers\n",
    "        self.graph_conv1 = GraphConvLayer(input_features, 64)\n",
    "        self.graph_conv2 = GraphConvLayer(64, 128)\n",
    "        self.graph_conv3 = GraphConvLayer(128, 256)\n",
    "        self.graph_conv4 = GraphConvLayer(256, 512)\n",
    "        \n",
    "        # Calculate flattened size based on sequence length\n",
    "        self.flatten_size = 512 * (sequence_length // 16)  # Due to pooling in graph conv layers\n",
    "        \n",
    "        # Dense layers with dropouts\n",
    "        self.dense = nn.Sequential(\n",
    "            nn.Linear(self.flatten_size, 4096),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.1),\n",
    "            nn.Linear(4096, 1024),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.1),\n",
    "            nn.Linear(1024, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.1),\n",
    "            nn.Linear(256, 2)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x, similarity_graphs):\n",
    "        # x shape: [batch_size, input_features, sequence_length]\n",
    "        x = self.graph_conv1(x)\n",
    "        x = self.graph_conv2(x)\n",
    "        x = self.graph_conv3(x)\n",
    "        x = self.graph_conv4(x)\n",
    "        \n",
    "        # Flatten\n",
    "        x = x.reshape(x.size(0), -1)\n",
    "        \n",
    "        # Dense layers\n",
    "        x = self.dense(x)\n",
    "        return x\n",
    "\n",
    "class GraphConvLayer(nn.Module):\n",
    "    def __init__(self, in_features, out_features):\n",
    "        super(GraphConvLayer, self).__init__()\n",
    "        \n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv1d(in_features, out_features, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool1d(2),\n",
    "            nn.BatchNorm1d(out_features)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # x shape: [batch_size, in_features, sequence_length]\n",
    "        x = self.conv(x)\n",
    "        return x\n",
    "\n",
    "def calculate_similarity_graphs(features, num_neighbors=9):\n",
    "    \"\"\"\n",
    "    Calculate similarity graphs between feature dimensions\n",
    "    features: [batch_size, features, sequence_length]\n",
    "    \"\"\"\n",
    "    # Average across sequence length to get feature correlations\n",
    "    features_mean = features.mean(dim=-1)  # [batch_size, features]\n",
    "    \n",
    "    num_features = features_mean.shape[1]\n",
    "    similarity_matrix = np.zeros((num_features, num_features))\n",
    "    \n",
    "    # Calculate correlation coefficients\n",
    "    features_np = features_mean.cpu().numpy()\n",
    "    for i in range(num_features):\n",
    "        for j in range(num_features):\n",
    "            correlation = np.corrcoef(features_np[:, i], features_np[:, j])[0, 1]\n",
    "            similarity_matrix[i, j] = abs(correlation)\n",
    "    \n",
    "    # For each feature, get top k similar features\n",
    "    similarity_graphs = []\n",
    "    for i in range(num_features):\n",
    "        # Get indices of top k similar features (excluding self)\n",
    "        similar_features = np.argsort(similarity_matrix[i])[-num_neighbors-1:-1]\n",
    "        similarity_graphs.append(similar_features)\n",
    "    \n",
    "    return similarity_graphs\n",
    "\n",
    "def train_model_three(model, train_loader, val_loader, num_epochs=200, learning_rate=0.001):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model = model.to(device)\n",
    "    \n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    \n",
    "    # Calculate similarity graphs once before training\n",
    "    features = next(iter(train_loader))[0]\n",
    "    similarity_graphs = calculate_similarity_graphs(features)\n",
    "    \n",
    "    best_val_acc = 0\n",
    "    patience = 20\n",
    "    patience_counter = 0\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        # Training\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        train_correct = 0\n",
    "        train_total = 0\n",
    "        \n",
    "        for features, labels in train_loader:\n",
    "            features, labels = features.to(device), labels.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(features, similarity_graphs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            train_loss += loss.item()\n",
    "            _, predicted = outputs.max(1)\n",
    "            train_total += labels.size(0)\n",
    "            train_correct += predicted.eq(labels).sum().item()\n",
    "        \n",
    "        # Validation\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        val_correct = 0\n",
    "        val_total = 0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for features, labels in val_loader:\n",
    "                features, labels = features.to(device), labels.to(device)\n",
    "                outputs = model(features, similarity_graphs)\n",
    "                loss = criterion(outputs, labels)\n",
    "                \n",
    "                val_loss += loss.item()\n",
    "                _, predicted = outputs.max(1)\n",
    "                val_total += labels.size(0)\n",
    "                val_correct += predicted.eq(labels).sum().item()\n",
    "        \n",
    "        val_acc = 100. * val_correct / val_total\n",
    "        \n",
    "        # Early stopping\n",
    "        if val_acc > best_val_acc:\n",
    "            best_val_acc = val_acc\n",
    "            patience_counter = 0\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            if patience_counter >= patience:\n",
    "                print(f\"Early stopping triggered at epoch {epoch+1}\")\n",
    "                break\n",
    "        \n",
    "        if (epoch + 1) % 10 == 0:\n",
    "            print(f'Epoch [{epoch+1}/{num_epochs}]')\n",
    "            print(f'Train Loss: {train_loss/len(train_loader):.4f}, '\n",
    "                  f'Train Acc: {100.*train_correct/train_total:.2f}%')\n",
    "            print(f'Val Loss: {val_loss/len(val_loader):.4f}, '\n",
    "                  f'Val Acc: {val_acc:.2f}%\\n')\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/200]\n",
      "Train Loss: 0.3096, Train Acc: 85.84%\n",
      "Val Loss: 1.5318, Val Acc: 50.53%\n",
      "\n",
      "Epoch [20/200]\n",
      "Train Loss: 0.1656, Train Acc: 93.12%\n",
      "Val Loss: 2.2440, Val Acc: 53.77%\n",
      "\n",
      "Epoch [30/200]\n",
      "Train Loss: 0.0938, Train Acc: 96.41%\n",
      "Val Loss: 2.9689, Val Acc: 50.83%\n",
      "\n",
      "Early stopping triggered at epoch 40\n"
     ]
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "# Initialize the model with your specific dimensions\n",
    "# first model \n",
    "# model = DepressionDetector1DCNN(input_features=40, sequence_length=SEQMENT_LENGTH)\n",
    "\n",
    "# second model\n",
    "# model = DepressionDetectorCNN(input_features=40, sequence_length=SEQMENT_LENGTH)\n",
    "\n",
    "# third model\n",
    "\n",
    "model = DepressionGCNN(input_features=40, sequence_length=SEQMENT_LENGTH)\n",
    "\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "# Create data loaders\n",
    "train_loader = DataLoader(dataset_train, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_loader = DataLoader(dataset_val, batch_size=BATCH_SIZE)\n",
    "\n",
    "# Use the training function as before\n",
    "# model = train_model(train_loader, train_loader, sequence_length=SEQMENT_LENGTH, num_epochs=200, batch_size=BATCH_SIZE, learning_rate=0.001)\n",
    "\n",
    "model = train_model_three(model, train_loader, val_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DepressionDetector1DCNN(nn.Module):\n",
    "    def __init__(self, input_features=40):\n",
    "        super(DepressionDetector1DCNN, self).__init__()\n",
    "        \n",
    "        self.conv_layers = nn.Sequential(\n",
    "            nn.Conv1d(input_features, 64, kernel_size=3, padding=1), # 64 ->> 500\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool1d(2),\n",
    "            nn.BatchNorm1d(64),\n",
    "            \n",
    "            nn.Conv1d(64, 128, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool1d(2),\n",
    "            nn.BatchNorm1d(128),\n",
    "            \n",
    "            nn.Conv1d(128, 256, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool1d(2),\n",
    "            nn.BatchNorm1d(256)\n",
    "        )\n",
    "        \n",
    "        self.flatten = nn.Flatten()\n",
    "        \n",
    "        # Calculate size after convolutions\n",
    "        self.dense_layers = nn.Sequential(\n",
    "            nn.Linear(256 * 8, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(64, 2)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.conv_layers(x)\n",
    "        x = self.flatten(x)\n",
    "        x = self.dense_layers(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# Assuming your DepressionDetector1DCNN and other classes are defined as shown earlier\n",
    "\n",
    "def train_model(dataloaders, num_epochs=200, learning_rate=0.001):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model = DepressionDetector1DCNN().to(device)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        for phase in ['train', 'test']:\n",
    "            if phase == 'train':\n",
    "                model.train()  # Set model to training mode\n",
    "            else:\n",
    "                model.eval()   # Set model to evaluate mode\n",
    "\n",
    "            running_loss = 0.0\n",
    "            running_corrects = 0\n",
    "\n",
    "            # Iterate over data.\n",
    "            for inputs, labels in dataloaders[phase]:\n",
    "                inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "                # zero the parameter gradients\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                # forward\n",
    "                with torch.set_grad_enabled(phase == 'train'):\n",
    "                    outputs = model(inputs)\n",
    "                    _, preds = torch.max(outputs, 1)\n",
    "                    loss = criterion(outputs, labels)\n",
    "\n",
    "                    # backward + optimize only if in training phase\n",
    "                    if phase == 'train':\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "\n",
    "                # statistics\n",
    "                running_loss += loss.item() * inputs.size(0)\n",
    "                running_corrects += torch.sum(preds == labels.data)\n",
    "\n",
    "            epoch_loss = running_loss / len(dataloaders[phase].dataset)\n",
    "            epoch_acc = running_corrects.double() / len(dataloaders[phase].dataset)\n",
    "\n",
    "            if (epoch + 1) % 10 == 0:\n",
    "                print(f'Epoch [{epoch+1}/{num_epochs}]')\n",
    "                print(f'{phase} Loss: {epoch_loss:.4f} Acc: {epoch_acc:.4f}')\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "\n",
    "class AudioDataset(Dataset):\n",
    "    def __init__(self, mfcc_features, labels):\n",
    "        self.features = torch.FloatTensor(mfcc_features)\n",
    "        self.labels = torch.LongTensor(labels)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        return self.features[idx], self.labels[idx]\n",
    "\n",
    "class DepressionDetector1DCNN(nn.Module):\n",
    "    def __init__(self, input_features=40):\n",
    "        super(DepressionDetector1DCNN, self).__init__()\n",
    "        \n",
    "        self.conv_layers = nn.Sequential(\n",
    "            nn.Conv1d(input_features, 64, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool1d(2),\n",
    "            nn.BatchNorm1d(64),\n",
    "            \n",
    "            nn.Conv1d(64, 128, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool1d(2),\n",
    "            nn.BatchNorm1d(128),\n",
    "            \n",
    "            nn.Conv1d(128, 256, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool1d(2),\n",
    "            nn.BatchNorm1d(256)\n",
    "        )\n",
    "        \n",
    "        self.flatten = nn.Flatten()\n",
    "        \n",
    "        # Calculate size after convolutions\n",
    "        self.dense_layers = nn.Sequential(\n",
    "            nn.Linear(256 * 8, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(64, 2)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.conv_layers(x)\n",
    "        x = self.flatten(x)\n",
    "        x = self.dense_layers(x)\n",
    "        return x\n",
    "\n",
    "def train_model(mfcc_features_train, labels_train, mfcc_features_val, labels_val, \n",
    "                num_epochs=200, batch_size=16, learning_rate=0.001):\n",
    "    \n",
    "    # Create datasets and dataloaders\n",
    "    train_dataset = AudioDataset(mfcc_features_train, labels_train)\n",
    "    val_dataset = AudioDataset(mfcc_features_val, labels_val)\n",
    "    \n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size)\n",
    "    \n",
    "    # Initialize model, loss function, and optimizer\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model = DepressionDetector1DCNN().to(device)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    \n",
    "    # Training loop\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        train_correct = 0\n",
    "        train_total = 0\n",
    "        \n",
    "        for features, labels in train_loader:\n",
    "            features, labels = features.to(device), labels.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(features)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            train_loss += loss.item()\n",
    "            _, predicted = outputs.max(1)\n",
    "            train_total += labels.size(0)\n",
    "            train_correct += predicted.eq(labels).sum().item()\n",
    "            \n",
    "        # Validation\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        val_correct = 0\n",
    "        val_total = 0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for features, labels in val_loader:\n",
    "                features, labels = features.to(device), labels.to(device)\n",
    "                outputs = model(features)\n",
    "                loss = criterion(outputs, labels)\n",
    "                \n",
    "                val_loss += loss.item()\n",
    "                _, predicted = outputs.max(1)\n",
    "                val_total += labels.size(0)\n",
    "                val_correct += predicted.eq(labels).sum().item()\n",
    "        \n",
    "        # Print epoch statistics\n",
    "        if (epoch + 1) % 10 == 0:\n",
    "            print(f'Epoch [{epoch+1}/{num_epochs}]')\n",
    "            print(f'Train Loss: {train_loss/len(train_loader):.4f}, '\n",
    "                  f'Train Acc: {100.*train_correct/train_total:.2f}%')\n",
    "            print(f'Val Loss: {val_loss/len(val_loader):.4f}, '\n",
    "                  f'Val Acc: {100.*val_correct/val_total:.2f}%\\n')\n",
    "    \n",
    "    return model\n",
    "\n",
    "def evaluate_model(model, mfcc_features_test, labels_test, batch_size=16):\n",
    "    test_dataset = AudioDataset(mfcc_features_test, labels_test)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size)\n",
    "    \n",
    "    device = next(model.parameters()).device\n",
    "    model.eval()\n",
    "    \n",
    "    correct = 0\n",
    "    total = 0\n",
    "    all_predictions = []\n",
    "    all_labels = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for features, labels in test_loader:\n",
    "            features, labels = features.to(device), labels.to(device)\n",
    "            outputs = model(features)\n",
    "            _, predicted = outputs.max(1)\n",
    "            \n",
    "            total += labels.size(0)\n",
    "            correct += predicted.eq(labels).sum().item()\n",
    "            \n",
    "            all_predictions.extend(predicted.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "    \n",
    "    accuracy = 100. * correct / total\n",
    "    print(f'Test Accuracy: {accuracy:.2f}%')\n",
    "    \n",
    "    return np.array(all_predictions), np.array(all_labels)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage:\n",
    "if __name__ == \"__main__\":\n",
    "    # Assuming your data is already preprocessed\n",
    "    # mfcc_features shape: (num_samples, num_features, time_steps)\n",
    "    # labels shape: (num_samples,)\n",
    "    \n",
    "    # Train the model\n",
    "    model = train_model(mfcc_features_train, labels_train, \n",
    "                       mfcc_features_val, labels_val)\n",
    "    \n",
    "    # Evaluate\n",
    "    predictions, true_labels = evaluate_model(model, mfcc_features_test, labels_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
