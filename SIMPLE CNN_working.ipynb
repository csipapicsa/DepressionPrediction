{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50 - Train Loss: 1.6405 - Train Acc: 51.00% - Val Loss: 0.2497 - Val Acc: 86.00%\n",
      "Epoch 2/50 - Train Loss: 0.4404 - Train Acc: 79.50% - Val Loss: 0.0526 - Val Acc: 98.50%\n",
      "Epoch 3/50 - Train Loss: 0.0857 - Train Acc: 96.50% - Val Loss: 0.0144 - Val Acc: 100.00%\n",
      "Epoch 4/50 - Train Loss: 0.0080 - Train Acc: 100.00% - Val Loss: 0.0039 - Val Acc: 100.00%\n",
      "Epoch 5/50 - Train Loss: 0.0035 - Train Acc: 100.00% - Val Loss: 0.0024 - Val Acc: 100.00%\n",
      "Epoch 6/50 - Train Loss: 0.0024 - Train Acc: 100.00% - Val Loss: 0.0020 - Val Acc: 100.00%\n",
      "Epoch 7/50 - Train Loss: 0.0020 - Train Acc: 100.00% - Val Loss: 0.0017 - Val Acc: 100.00%\n",
      "Epoch 8/50 - Train Loss: 0.0017 - Train Acc: 100.00% - Val Loss: 0.0015 - Val Acc: 100.00%\n",
      "Epoch 9/50 - Train Loss: 0.0015 - Train Acc: 100.00% - Val Loss: 0.0013 - Val Acc: 100.00%\n",
      "Epoch 10/50 - Train Loss: 0.0013 - Train Acc: 100.00% - Val Loss: 0.0012 - Val Acc: 100.00%\n",
      "Epoch 11/50 - Train Loss: 0.0012 - Train Acc: 100.00% - Val Loss: 0.0010 - Val Acc: 100.00%\n",
      "Epoch 12/50 - Train Loss: 0.0010 - Train Acc: 100.00% - Val Loss: 0.0009 - Val Acc: 100.00%\n",
      "Epoch 13/50 - Train Loss: 0.0009 - Train Acc: 100.00% - Val Loss: 0.0009 - Val Acc: 100.00%\n",
      "Epoch 14/50 - Train Loss: 0.0009 - Train Acc: 100.00% - Val Loss: 0.0008 - Val Acc: 100.00%\n",
      "Epoch 15/50 - Train Loss: 0.0008 - Train Acc: 100.00% - Val Loss: 0.0007 - Val Acc: 100.00%\n",
      "Epoch 16/50 - Train Loss: 0.0007 - Train Acc: 100.00% - Val Loss: 0.0007 - Val Acc: 100.00%\n",
      "Epoch 17/50 - Train Loss: 0.0006 - Train Acc: 100.00% - Val Loss: 0.0006 - Val Acc: 100.00%\n",
      "Epoch 18/50 - Train Loss: 0.0006 - Train Acc: 100.00% - Val Loss: 0.0006 - Val Acc: 100.00%\n",
      "Epoch 19/50 - Train Loss: 0.0006 - Train Acc: 100.00% - Val Loss: 0.0005 - Val Acc: 100.00%\n",
      "Epoch 20/50 - Train Loss: 0.0005 - Train Acc: 100.00% - Val Loss: 0.0005 - Val Acc: 100.00%\n",
      "Epoch 21/50 - Train Loss: 0.0005 - Train Acc: 100.00% - Val Loss: 0.0004 - Val Acc: 100.00%\n",
      "Epoch 22/50 - Train Loss: 0.0004 - Train Acc: 100.00% - Val Loss: 0.0004 - Val Acc: 100.00%\n",
      "Epoch 23/50 - Train Loss: 0.0004 - Train Acc: 100.00% - Val Loss: 0.0004 - Val Acc: 100.00%\n",
      "Epoch 24/50 - Train Loss: 0.0004 - Train Acc: 100.00% - Val Loss: 0.0004 - Val Acc: 100.00%\n",
      "Epoch 25/50 - Train Loss: 0.0004 - Train Acc: 100.00% - Val Loss: 0.0003 - Val Acc: 100.00%\n",
      "Epoch 26/50 - Train Loss: 0.0003 - Train Acc: 100.00% - Val Loss: 0.0003 - Val Acc: 100.00%\n",
      "Epoch 27/50 - Train Loss: 0.0003 - Train Acc: 100.00% - Val Loss: 0.0003 - Val Acc: 100.00%\n",
      "Epoch 28/50 - Train Loss: 0.0003 - Train Acc: 100.00% - Val Loss: 0.0003 - Val Acc: 100.00%\n",
      "Epoch 29/50 - Train Loss: 0.0003 - Train Acc: 100.00% - Val Loss: 0.0003 - Val Acc: 100.00%\n",
      "Epoch 30/50 - Train Loss: 0.0003 - Train Acc: 100.00% - Val Loss: 0.0002 - Val Acc: 100.00%\n",
      "Epoch 31/50 - Train Loss: 0.0002 - Train Acc: 100.00% - Val Loss: 0.0002 - Val Acc: 100.00%\n",
      "Epoch 32/50 - Train Loss: 0.0002 - Train Acc: 100.00% - Val Loss: 0.0002 - Val Acc: 100.00%\n",
      "Epoch 33/50 - Train Loss: 0.0002 - Train Acc: 100.00% - Val Loss: 0.0002 - Val Acc: 100.00%\n",
      "Epoch 34/50 - Train Loss: 0.0002 - Train Acc: 100.00% - Val Loss: 0.0002 - Val Acc: 100.00%\n",
      "Epoch 35/50 - Train Loss: 0.0002 - Train Acc: 100.00% - Val Loss: 0.0002 - Val Acc: 100.00%\n",
      "Epoch 36/50 - Train Loss: 0.0002 - Train Acc: 100.00% - Val Loss: 0.0002 - Val Acc: 100.00%\n",
      "Epoch 37/50 - Train Loss: 0.0002 - Train Acc: 100.00% - Val Loss: 0.0002 - Val Acc: 100.00%\n",
      "Epoch 38/50 - Train Loss: 0.0002 - Train Acc: 100.00% - Val Loss: 0.0001 - Val Acc: 100.00%\n",
      "Epoch 39/50 - Train Loss: 0.0001 - Train Acc: 100.00% - Val Loss: 0.0001 - Val Acc: 100.00%\n",
      "Epoch 40/50 - Train Loss: 0.0001 - Train Acc: 100.00% - Val Loss: 0.0001 - Val Acc: 100.00%\n",
      "Epoch 41/50 - Train Loss: 0.0001 - Train Acc: 100.00% - Val Loss: 0.0001 - Val Acc: 100.00%\n",
      "Epoch 42/50 - Train Loss: 0.0001 - Train Acc: 100.00% - Val Loss: 0.0001 - Val Acc: 100.00%\n",
      "Epoch 43/50 - Train Loss: 0.0001 - Train Acc: 100.00% - Val Loss: 0.0001 - Val Acc: 100.00%\n",
      "Epoch 44/50 - Train Loss: 0.0001 - Train Acc: 100.00% - Val Loss: 0.0001 - Val Acc: 100.00%\n",
      "Epoch 45/50 - Train Loss: 0.0001 - Train Acc: 100.00% - Val Loss: 0.0001 - Val Acc: 100.00%\n",
      "Epoch 46/50 - Train Loss: 0.0001 - Train Acc: 100.00% - Val Loss: 0.0001 - Val Acc: 100.00%\n",
      "Epoch 47/50 - Train Loss: 0.0001 - Train Acc: 100.00% - Val Loss: 0.0001 - Val Acc: 100.00%\n",
      "Epoch 48/50 - Train Loss: 0.0001 - Train Acc: 100.00% - Val Loss: 0.0001 - Val Acc: 100.00%\n",
      "Epoch 49/50 - Train Loss: 0.0001 - Train Acc: 100.00% - Val Loss: 0.0001 - Val Acc: 100.00%\n",
      "Epoch 50/50 - Train Loss: 0.0001 - Train Acc: 100.00% - Val Loss: 0.0001 - Val Acc: 100.00%\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(Simple1DCNN(\n",
       "   (conv1): Conv1d(40, 64, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "   (pool1): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "   (flatten): Flatten(start_dim=1, end_dim=-1)\n",
       "   (fc): Linear(in_features=16000, out_features=2, bias=True)\n",
       " ),\n",
       " [0.51,\n",
       "  0.795,\n",
       "  0.965,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0],\n",
       " [0.86,\n",
       "  0.985,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0],\n",
       " [1.6404701419384218,\n",
       "  0.44037785388948353,\n",
       "  0.08573928042327679,\n",
       "  0.007998312463023467,\n",
       "  0.0034526509336774325,\n",
       "  0.002366503458015359,\n",
       "  0.0020012091844910174,\n",
       "  0.0017156378643812787,\n",
       "  0.0014944151081817836,\n",
       "  0.001288485798122565,\n",
       "  0.0011638405474150204,\n",
       "  0.0010362818059547862,\n",
       "  0.0009288790862683527,\n",
       "  0.0008587470856946311,\n",
       "  0.0007764615578571465,\n",
       "  0.0007075691785757953,\n",
       "  0.000647683809256705,\n",
       "  0.000598170464472787,\n",
       "  0.000552349400568346,\n",
       "  0.0005091172635184193,\n",
       "  0.0004743734884050355,\n",
       "  0.00043891653751416016,\n",
       "  0.00040744371849768865,\n",
       "  0.0003807570914614189,\n",
       "  0.0003533106611803305,\n",
       "  0.0003308707790347398,\n",
       "  0.0003064937936142087,\n",
       "  0.0002890541899660093,\n",
       "  0.00027199127078347373,\n",
       "  0.00025394448812221526,\n",
       "  0.00023702328763192782,\n",
       "  0.0002243986972462153,\n",
       "  0.00020932648187454105,\n",
       "  0.0001968650896287727,\n",
       "  0.0001848117615281808,\n",
       "  0.00017219874492184318,\n",
       "  0.00016630548651846766,\n",
       "  0.00015389108424642472,\n",
       "  0.0001450248672699672,\n",
       "  0.00013708111092000763,\n",
       "  0.00012902230593340392,\n",
       "  0.00012293227456211753,\n",
       "  0.00011597106019507919,\n",
       "  0.0001086474246585567,\n",
       "  0.00010313779263015022,\n",
       "  9.698265079350676e-05,\n",
       "  9.139480293924862e-05,\n",
       "  8.648516661764916e-05,\n",
       "  8.175365485385555e-05,\n",
       "  7.7391669183271e-05],\n",
       " [0.2496809584303992,\n",
       "  0.052552990156073065,\n",
       "  0.014401413856212457,\n",
       "  0.003922349479871627,\n",
       "  0.002406018868823594,\n",
       "  0.0020183126624760917,\n",
       "  0.001690366678958526,\n",
       "  0.0014903194954604259,\n",
       "  0.0013037373304905487,\n",
       "  0.0011653473313526775,\n",
       "  0.0010481790369522058,\n",
       "  0.0009402943020904785,\n",
       "  0.000868145396525506,\n",
       "  0.0007777419480589742,\n",
       "  0.0007152350162141374,\n",
       "  0.0006534777784327161,\n",
       "  0.000602309910209442,\n",
       "  0.0005571564359433978,\n",
       "  0.0005116258536145324,\n",
       "  0.0004823900074552512,\n",
       "  0.0004394607948961493,\n",
       "  0.00040854200004105225,\n",
       "  0.00038247417940510785,\n",
       "  0.00035775169164480757,\n",
       "  0.00033183089114572793,\n",
       "  0.00031063211983564545,\n",
       "  0.00029012958460953084,\n",
       "  0.0002736809114867356,\n",
       "  0.0002547039007004059,\n",
       "  0.0002405295047446998,\n",
       "  0.00022596733287173266,\n",
       "  0.00021234260821984208,\n",
       "  0.00019777924774643906,\n",
       "  0.00018578253417217639,\n",
       "  0.00017495927522759303,\n",
       "  0.0001670759304852254,\n",
       "  0.00015644507122487993,\n",
       "  0.00014667434880720974,\n",
       "  0.00013836427881869896,\n",
       "  0.00013080480392090976,\n",
       "  0.00012326006596595108,\n",
       "  0.00011580068601688254,\n",
       "  0.00010914807081462641,\n",
       "  0.00010320035356016888,\n",
       "  9.74069916537701e-05,\n",
       "  9.208315513660637e-05,\n",
       "  8.69691137359041e-05,\n",
       "  8.265531196229858e-05,\n",
       "  7.768906800265541e-05,\n",
       "  7.342299299580191e-05])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class AudioDataset(Dataset):\n",
    "    def __init__(self, mfcc_features, labels):\n",
    "        self.features = torch.FloatTensor(mfcc_features)\n",
    "        self.labels = torch.LongTensor(labels)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        return self.features[idx], self.labels[idx]\n",
    "\n",
    "class Simple1DCNN(nn.Module):\n",
    "    def __init__(self, input_features=40, input_length=150):\n",
    "        super(Simple1DCNN, self).__init__()\n",
    "        self.conv1 = nn.Conv1d(input_features, 64, kernel_size=3, stride=1, padding=1)\n",
    "        self.pool1 = nn.MaxPool1d(kernel_size=2, stride=2)\n",
    "        # Calculate size after first Conv and Pool\n",
    "        self.size_after_conv1 = (input_length + 2*1 - 3) // 1 + 1\n",
    "        self.size_after_pool1 = (self.size_after_conv1 - 2) // 2 + 1\n",
    "\n",
    "        self.flatten = nn.Flatten()\n",
    "        # Adjust the linear layer size\n",
    "        self.fc = nn.Linear(64 * self.size_after_pool1, 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool1(nn.ReLU()(self.conv1(x)))\n",
    "        x = self.flatten(x)\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "\n",
    "def train_model(model, train_loader, val_loader, num_epochs=50, learning_rate=0.001):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model = model.to(device)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    train_acc, val_acc = [], []\n",
    "    train_losses, val_losses = [], []\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        correct, total = 0, 0\n",
    "\n",
    "        for data in train_loader:\n",
    "            inputs, labels = data[0].to(device), data[1].to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "        train_losses.append(running_loss / len(train_loader))\n",
    "        train_acc.append(correct / total)\n",
    "\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        correct, total = 0, 0\n",
    "        with torch.no_grad():\n",
    "            for data in val_loader:\n",
    "                inputs, labels = data[0].to(device), data[1].to(device)\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, labels)\n",
    "                val_loss += loss.item()\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                total += labels.size(0)\n",
    "                correct += (predicted == labels).sum().item()\n",
    "\n",
    "        val_losses.append(val_loss / len(val_loader))\n",
    "        val_acc.append(correct / total)\n",
    "\n",
    "        print(f'Epoch {epoch+1}/{num_epochs} - Train Loss: {train_losses[-1]:.4f} - Train Acc: {train_acc[-1]*100:.2f}% - Val Loss: {val_losses[-1]:.4f} - Val Acc: {val_acc[-1]*100:.2f}%')\n",
    "\n",
    "    return model, train_acc, val_acc, train_losses, val_losses\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "303 0\n",
      "304 0\n",
      "305 0\n",
      "310 0\n",
      "312 0\n",
      "313 0\n",
      "315 0\n",
      "316 0\n",
      "317 0\n",
      "318 0\n",
      "319 1\n",
      "320 1\n",
      "321 1\n",
      "322 0\n",
      "324 0\n",
      "325 1\n",
      "326 0\n",
      "327 0\n",
      "328 0\n",
      "330 1\n",
      "333 0\n",
      "336 0\n",
      "338 1\n",
      "339 1\n",
      "340 0\n",
      "341 0\n",
      "343 0\n",
      "344 1\n",
      "345 1\n",
      "347 1\n",
      "348 1\n",
      "350 1\n",
      "351 1\n",
      "352 1\n",
      "353 1\n",
      "355 1\n",
      "356 1\n",
      "357 0\n",
      "358 0\n",
      "360 0\n",
      "362 1\n",
      "363 0\n",
      "364 0\n",
      "366 0\n",
      "368 0\n",
      "369 0\n",
      "370 0\n",
      "371 0\n",
      "372 1\n",
      "374 0\n",
      "375 0\n",
      "376 1\n",
      "379 0\n",
      "380 1\n",
      "383 0\n",
      "385 0\n",
      "386 1\n",
      "391 0\n",
      "392 0\n",
      "393 0\n",
      "397 0\n",
      "400 0\n",
      "401 0\n",
      "402 1\n",
      "409 0\n",
      "412 1\n",
      "414 1\n",
      "415 0\n",
      "416 0\n",
      "419 0\n",
      "423 0\n",
      "425 0\n",
      "426 1\n",
      "427 0\n",
      "428 0\n",
      "429 0\n",
      "430 0\n",
      "433 1\n",
      "434 0\n",
      "437 0\n",
      "441 1\n",
      "443 0\n",
      "444 0\n",
      "445 0\n",
      "446 0\n",
      "447 0\n",
      "448 1\n",
      "449 0\n",
      "454 0\n",
      "455 0\n",
      "456 0\n",
      "457 0\n",
      "459 1\n",
      "463 0\n",
      "464 0\n",
      "468 0\n",
      "471 0\n",
      "473 0\n",
      "474 0\n",
      "475 0\n",
      "478 0\n",
      "479 0\n",
      "485 0\n",
      "486 0\n",
      "487 0\n",
      "488 0\n",
      "491 0\n",
      "Failed to process patient 491: [Errno 2] No such file or directory: 'datasets/DAIC-WOZ/ConcatenatedMFCC/concatenated_mfcc_491.npz'\n",
      "302 0\n",
      "307 0\n",
      "331 0\n",
      "335 1\n",
      "346 1\n",
      "367 1\n",
      "377 1\n",
      "381 1\n",
      "382 0\n",
      "388 1\n",
      "389 1\n",
      "390 0\n",
      "395 0\n",
      "403 0\n",
      "404 0\n",
      "406 0\n",
      "413 1\n",
      "417 0\n",
      "418 1\n",
      "420 0\n",
      "422 1\n",
      "436 0\n",
      "439 0\n",
      "440 1\n",
      "451 0\n",
      "458 0\n",
      "472 0\n",
      "476 0\n",
      "477 0\n",
      "482 0\n",
      "483 1\n",
      "484 0\n",
      "489 0\n",
      "Failed to process patient 489: [Errno 2] No such file or directory: 'datasets/DAIC-WOZ/ConcatenatedMFCC/concatenated_mfcc_489.npz'\n",
      "490 0\n",
      "Failed to process patient 490: [Errno 2] No such file or directory: 'datasets/DAIC-WOZ/ConcatenatedMFCC/concatenated_mfcc_490.npz'\n",
      "492 0\n",
      "Failed to process patient 492: [Errno 2] No such file or directory: 'datasets/DAIC-WOZ/ConcatenatedMFCC/concatenated_mfcc_492.npz'\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('datasets/DAIC-WOZ/Patient_Classes.csv')\n",
    "\n",
    "def segment_mfcc(mfcc, max_segment_length=200):\n",
    "    # Segment the MFCC array into fixed lengths with possible overlap (if desired)\n",
    "    # segment_length is the fixed length of each segment\n",
    "    segments = []\n",
    "    for start in range(0, mfcc.shape[1], max_segment_length):\n",
    "        end = start + max_segment_length\n",
    "        if end < mfcc.shape[1]:\n",
    "            segments.append(mfcc[:, start:end])\n",
    "        else:\n",
    "            # Padding the last segment if it's shorter than the required segment length\n",
    "            segments.append(np.pad(mfcc[:, start:], ((0,0), (0, max_segment_length - (mfcc.shape[1] - start))), 'constant'))\n",
    "    return segments\n",
    "\n",
    "def load_concatenated_mfcc(path):\n",
    "    \n",
    "    # Load the concatenated MFCC data\n",
    "    data = np.load(path)\n",
    "    return data['mfcc']\n",
    "\n",
    "def create_datasets(df, max_segment_length=500):\n",
    "    dataset = {'train': [], 'test': []}\n",
    "    labels = {'train': [], 'test': []}\n",
    "\n",
    "    for _, row in df.iterrows():\n",
    "        patient_id = row['Participant_ID']\n",
    "        label = row['PHQ8_Binary']\n",
    "        print(patient_id, label)\n",
    "        train_or_test = row['dataset']  # Could be 'train' or 'test'\n",
    "        # if train_or_test is 'dev' change it to 'test'\n",
    "        if train_or_test == 'dev':\n",
    "            train_or_test = 'test'\n",
    "\n",
    "\n",
    "        mfcc_path = f'datasets/DAIC-WOZ/ConcatenatedMFCC/concatenated_mfcc_{patient_id}.npz'\n",
    "\n",
    "        try:\n",
    "            mfcc = load_concatenated_mfcc(mfcc_path)  # Load the raw MFCC data\n",
    "            segments = segment_mfcc(mfcc, max_segment_length=max_segment_length)\n",
    "            \n",
    "            # Append each segment to the corresponding dataset\n",
    "            for segment in segments:\n",
    "                dataset[train_or_test].append(segment)\n",
    "                labels[train_or_test].append(label)\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to process patient {patient_id}: {e}\")\n",
    "\n",
    "    return dataset, labels\n",
    "\n",
    "\n",
    "MAX_SEGMENT_LENGTH = 200\n",
    "dataset, labels = create_datasets(df, max_segment_length=MAX_SEGMENT_LENGTH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Gergo Gyori\\AppData\\Local\\Temp\\ipykernel_4720\\552959111.py:3: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ..\\torch\\csrc\\utils\\tensor_new.cpp:277.)\n",
      "  self.features = torch.FloatTensor(mfcc_features)\n"
     ]
    }
   ],
   "source": [
    "dataset = AudioDataset(dataset[\"train\"], labels[\"test\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = DataLoader(dataset, batch_size=2, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50 - Train Loss: 1.2197 - Train Acc: 61.88% - Val Loss: 0.5791 - Val Acc: 69.76%\n",
      "Epoch 2/50 - Train Loss: 0.6191 - Train Acc: 66.94% - Val Loss: 0.5883 - Val Acc: 68.94%\n",
      "Epoch 3/50 - Train Loss: 0.6023 - Train Acc: 67.78% - Val Loss: 0.5789 - Val Acc: 68.57%\n",
      "Epoch 4/50 - Train Loss: 0.5902 - Train Acc: 68.81% - Val Loss: 0.5424 - Val Acc: 71.36%\n",
      "Epoch 5/50 - Train Loss: 0.5852 - Train Acc: 70.54% - Val Loss: 0.6216 - Val Acc: 65.96%\n",
      "Epoch 6/50 - Train Loss: 0.5721 - Train Acc: 69.39% - Val Loss: 0.5219 - Val Acc: 73.55%\n",
      "Epoch 7/50 - Train Loss: 0.5551 - Train Acc: 71.55% - Val Loss: 0.5115 - Val Acc: 73.39%\n",
      "Epoch 8/50 - Train Loss: 0.5530 - Train Acc: 71.62% - Val Loss: 0.5590 - Val Acc: 68.25%\n",
      "Epoch 9/50 - Train Loss: 0.5468 - Train Acc: 73.12% - Val Loss: 0.5198 - Val Acc: 74.94%\n",
      "Epoch 10/50 - Train Loss: 0.5310 - Train Acc: 73.07% - Val Loss: 0.5080 - Val Acc: 74.07%\n",
      "Epoch 11/50 - Train Loss: 0.5089 - Train Acc: 74.28% - Val Loss: 0.5220 - Val Acc: 74.57%\n",
      "Epoch 12/50 - Train Loss: 0.5368 - Train Acc: 73.12% - Val Loss: 0.5028 - Val Acc: 73.55%\n",
      "Epoch 13/50 - Train Loss: 0.5382 - Train Acc: 74.07% - Val Loss: 0.5170 - Val Acc: 77.28%\n",
      "Epoch 14/50 - Train Loss: 0.4923 - Train Acc: 75.15% - Val Loss: 0.4757 - Val Acc: 73.91%\n",
      "Epoch 15/50 - Train Loss: 0.4838 - Train Acc: 76.05% - Val Loss: 0.6174 - Val Acc: 74.36%\n",
      "Epoch 16/50 - Train Loss: 0.5257 - Train Acc: 74.28% - Val Loss: 0.4529 - Val Acc: 76.52%\n",
      "Epoch 17/50 - Train Loss: 0.5091 - Train Acc: 74.49% - Val Loss: 0.4724 - Val Acc: 76.49%\n",
      "Epoch 18/50 - Train Loss: 0.4826 - Train Acc: 75.89% - Val Loss: 0.4756 - Val Acc: 77.52%\n",
      "Epoch 19/50 - Train Loss: 0.4790 - Train Acc: 76.63% - Val Loss: 0.4490 - Val Acc: 77.20%\n",
      "Epoch 20/50 - Train Loss: 0.4857 - Train Acc: 76.99% - Val Loss: 0.4792 - Val Acc: 74.81%\n",
      "Epoch 21/50 - Train Loss: 0.5160 - Train Acc: 75.28% - Val Loss: 0.4626 - Val Acc: 77.44%\n",
      "Epoch 22/50 - Train Loss: 0.4687 - Train Acc: 77.65% - Val Loss: 0.4302 - Val Acc: 78.36%\n",
      "Epoch 23/50 - Train Loss: 0.4451 - Train Acc: 78.94% - Val Loss: 0.4301 - Val Acc: 78.44%\n",
      "Epoch 24/50 - Train Loss: 0.4756 - Train Acc: 78.86% - Val Loss: 0.4090 - Val Acc: 79.18%\n",
      "Epoch 25/50 - Train Loss: 0.4372 - Train Acc: 79.02% - Val Loss: 0.4157 - Val Acc: 81.28%\n",
      "Epoch 26/50 - Train Loss: 0.4424 - Train Acc: 79.31% - Val Loss: 0.4696 - Val Acc: 79.44%\n",
      "Epoch 27/50 - Train Loss: 0.4489 - Train Acc: 79.63% - Val Loss: 0.4163 - Val Acc: 80.60%\n",
      "Epoch 28/50 - Train Loss: 0.4328 - Train Acc: 78.89% - Val Loss: 0.4059 - Val Acc: 80.65%\n",
      "Epoch 29/50 - Train Loss: 0.4667 - Train Acc: 79.99% - Val Loss: 0.4021 - Val Acc: 81.23%\n",
      "Epoch 30/50 - Train Loss: 0.4793 - Train Acc: 79.10% - Val Loss: 0.4110 - Val Acc: 79.71%\n",
      "Epoch 31/50 - Train Loss: 0.4162 - Train Acc: 79.94% - Val Loss: 0.5038 - Val Acc: 79.39%\n",
      "Epoch 32/50 - Train Loss: 0.4525 - Train Acc: 80.26% - Val Loss: 0.4008 - Val Acc: 80.13%\n",
      "Epoch 33/50 - Train Loss: 0.4611 - Train Acc: 80.31% - Val Loss: 0.3936 - Val Acc: 81.13%\n",
      "Epoch 34/50 - Train Loss: 0.4108 - Train Acc: 80.60% - Val Loss: 0.3683 - Val Acc: 82.39%\n",
      "Epoch 35/50 - Train Loss: 0.4578 - Train Acc: 81.55% - Val Loss: 0.3866 - Val Acc: 81.28%\n",
      "Epoch 36/50 - Train Loss: 0.4150 - Train Acc: 81.05% - Val Loss: 0.3671 - Val Acc: 83.36%\n",
      "Epoch 37/50 - Train Loss: 0.4324 - Train Acc: 80.84% - Val Loss: 0.3929 - Val Acc: 80.15%\n",
      "Epoch 38/50 - Train Loss: 0.4318 - Train Acc: 80.60% - Val Loss: 0.3603 - Val Acc: 82.39%\n",
      "Epoch 39/50 - Train Loss: 0.4431 - Train Acc: 80.89% - Val Loss: 0.3757 - Val Acc: 81.36%\n",
      "Epoch 40/50 - Train Loss: 0.4273 - Train Acc: 80.68% - Val Loss: 0.3665 - Val Acc: 82.05%\n",
      "Epoch 41/50 - Train Loss: 0.4199 - Train Acc: 80.92% - Val Loss: 0.3747 - Val Acc: 81.60%\n",
      "Epoch 42/50 - Train Loss: 0.4386 - Train Acc: 81.34% - Val Loss: 0.4177 - Val Acc: 80.31%\n",
      "Epoch 43/50 - Train Loss: 0.4447 - Train Acc: 81.13% - Val Loss: 0.3749 - Val Acc: 82.39%\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = Simple1DCNN(input_length=MAX_SEGMENT_LENGTH).to(device)\n",
    "#criterion = nn.CrossEntropyLoss()\n",
    "#optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "train_model(model, loader, loader, num_epochs=50, learning_rate=0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random dummy for development purposes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    # Dummy data (10 samples, 40 MFCC features, varying length of 150)\n",
    "PARTICIPANTS = 200 # automatically has to be defined \n",
    "MFCC_LENGTH = 500 # I have to control the numbeer of MFCC features (so like cutting or whatever)\n",
    "\n",
    "mfcc_features = np.random.randn(PARTICIPANTS, 40, MFCC_LENGTH) # 10 pcs of 40 MFCC features with length of 150\n",
    "# (200, 40, 500) \n",
    "labels = np.random.randint(0, 2, size=(PARTICIPANTS,)) # so the labels 0 2 means labels between 0 and 1\n",
    "\n",
    "dataset = AudioDataset(mfcc_features, labels)\n",
    "loader = DataLoader(dataset, batch_size=2, shuffle=True)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = Simple1DCNN(input_length=MFCC_LENGTH).to(device)\n",
    "#criterion = nn.CrossEntropyLoss()\n",
    "#optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "train_model(model, loader, loader, num_epochs=50, learning_rate=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
